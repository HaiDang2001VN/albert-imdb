{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzmyfWAy0pJw"
      },
      "source": [
        "# VLSP Dataset Preparation\n",
        "\n",
        "This notebook is a walkthrough of our preprocessing step for the VLSP2020 Relation Extraction dataset (view more [here](https://vlsp.org.vn/vlsp2020/eval/re)). Preprocessing this dataset is quite a challenge due to many points that need fixing in the annotation. Additionally, to meet the best setting for the experiment, we propose two different input formats. At the end of this step, a folder named `vlsp_preprocessed` is created with two subfolders for two input formats, each containing three datasets (train, dev, and test) as json files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing Environment\n",
        "\n",
        "PhoBERT requires Vietnamese sentences to be word-segmented by VnCoreNLP in advanced. Therefore, we need to set up the model and load it for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4qltZcTHBKS",
        "outputId": "081cc2c9-ad87-4d25-a4bc-d9ec3da9fba7"
      },
      "outputs": [],
      "source": [
        "import py_vncorenlp\n",
        "import os\n",
        "\n",
        "vncorenlp_dir = \"D:/Projects/albert-imdb/vncorenlp\"\n",
        "project_dir = \"D:/Projects/albert-imdb\"\n",
        "\n",
        "# uncomment this line if VnCoreNLP has not been downloaded\n",
        "# py_vncorenlp.download_model(save_dir=vncorenlp_path)\n",
        "\n",
        "# load VnCoreNLP\n",
        "vncorenlp_model = py_vncorenlp.VnCoreNLP(save_dir=vncorenlp_dir, annotators=[\"wseg\"])\n",
        "\n",
        "# change directory back to the project\n",
        "os.chdir(project_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessor Implementation\n",
        "\n",
        "The whole process includes two main stages:\n",
        "\n",
        "1. **Loading stage**: The preprocessor loads all tsv files in a directory and processes each at a time as a dataframe. For convenience, we rename the columns as `ann_idx`, `range`, `word`, `var`, `entity`, `relation`, and `rel_heads`. Next, we extract sentences denoting roughly by a word ending with `.`, `?`, or `!`. Finally, for each relation detected in the dataframe, we save it with the entities and their ranges in attachment of the corresponding sentence.\n",
        "\n",
        "2. **Formatting stage**: For each relation, the preprocessor adds one more sample to `preprocessor.sentences`. The sentence will be formatted based on the format code provided (view our report for more details). If the `run_vncorenlp_wseg` is on, the sentence will continue to be segmented. For the special tokens inserted into the sentence beforehand, we simply substitute the segmented tokens with the original ones.\n",
        "\n",
        "After executing all, there is also an optional stage of shuffling the model-ready samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 7 annotation errors (some might not actually be errors but here we use \"error\" as a general term for all mentioned circumstances) that needs fixing:\n",
        "\n",
        "1. **Entity as a subword**: Normally, the annotation splits each word into each row by space. However, when a word is followed immediately by a punctuation and that word is a part of an entity, the annotator will duplicate that word into the next row. This sometimes happens with a \"word\" actually containing multiple words and the entity word is a part of that.\n",
        "```\n",
        "# VLSP2020_RE_train\\23357000.conll\n",
        "1-173\t807-810\tvăn\t*[8]\tORGANIZATION[8]\tAFFILIATION\t1-166[7_8]\n",
        "1-174\t811-816\tphòng\t*[8]\tORGANIZATION[8]\t_\t_\n",
        "1-175\t817-821\tkiến\t*[8]\tORGANIZATION[8]\t_\t_\n",
        "1-176\t822-826\ttrúc\t*[8]\tORGANIZATION[8]\t_\t_\n",
        "1-177\t827-833\t1+1>2;\t_\t_\t_\t_\n",
        "1-177.1\t827-832\t1+1>2\t*[8]\tORGANIZATION[8]\tAFFILIATION\t1-166[7_8]\n",
        "```\n",
        "\n",
        "2. **Not separated by space**: There are some cases that the annotator left two words or more with spaces in between in a row and an entity part is in that chunk (normally at the end or beginning). In order to make the data meaningful, we have to separate that entity part out. However, the difficult part is that we cannot detect which part belongs to an entity. Until the time writing this, we have only examined one file with this error. However, that file is also heavily corrupted with many other problems so we decided to ignore the whole file. Note that there are cases similar in structure but the entity part is duplicated into the next row. Those cases count as the first error.\n",
        "```\n",
        "# VLSP2020_RE_train\\23352816.conll\n",
        "1-153\t756-763\t. VMISS\t*\tORGANIZATION\t_\t_\t_\n",
        "```\n",
        "\n",
        "3. **Inter-sentence relations**: The task is intra-sentence but there are some relations between entities belong to different sentences. Therefore, we have to get rid of those relations. This is addressed by checking if the index of the pointed entity is in the same sentence with the origin entity.\n",
        "\n",
        "4. **Relation annotation not only in the first row of the entity**: If an entity is annotated with a relation, that relation should be inserted to the first row of that entity. But there are some cases, such as when getting the entity with punctuation error (see the example in that case), the entity annotation is interrupted and then continue with reinserting the relation.\n",
        "\n",
        "5. **Relation not linking to the first word of the entity**: A relation should be linked to the first word of the other entity but when the other entity annotation is interrupted by the first error, the relation is linked to the row of the duplicated word with no punctuation. Due to this error, when finding the range of the entities, we have to go up and down the dataframe from the referenced index to check it.\n",
        "```\n",
        "# VLSP2020_RE_train\\23356574.conll\n",
        "1-742\t3340-3346\tchuyên\t*[18]\tORGANIZATION[18]\t_\t_\n",
        "1-743\t3347-3354\tnghiệp,\t_\t_\t_\t_\n",
        "1-743.1\t3347-3353\tnghiệp\t*[18]\tORGANIZATION[18]\tPART – WHOLE\t1-733[17_18]\n",
        "1-744\t3355-3357\tSở\t*[19]\tORGANIZATION[19]\tAFFILIATION|PART – WHOLE\t1-733[17_19]|1-743.1[18_19]\n",
        "1-745\t3358-3363\tGD-ĐT\t*[19]\tORGANIZATION[19]\t_\t_\n",
        "```\n",
        "\n",
        "6. **Miscellaneous entities and relations**: Some places in a dataframe, the annotation only detect there is some entity or relation but not specify which that is. For convenience, we ignore all miscellaneous entities and relations, as well as relations involving miscellaneous entities.\n",
        "```\n",
        "# VLSP2020_RE_train\\23366765.conll\n",
        "1-6\t30-35\tApple\t*[1]\tMISCELLANEOUS[1]\t\n",
        "1-7\t36-39\tPay\t*[1]\tMISCELLANEOUS[1]\t\n",
        "# VLSP2020_RE_train\\23351515.conll\n",
        "1-318\t1372-1376\ttỉnh\t*[19]\tLOCATION[19]\tPART – WHOLE|LOCATED|PART – WHOLE|*\t1-315[18_19]|1-301[15_19]|1-307[16_19]|1-310[17_19]\t\n",
        "1-319\t1377-1381\tBình\t*[19]\tLOCATION[19]\t_\t_\t\n",
        "1-320\t1382-1386\tĐịnh\t*[19]\tLOCATION[19]\t_\t_\t\n",
        "```\n",
        "\n",
        "7. **Same name for different entities**: Normally, if there are two or more entities with the same type, there will be an index after the entity type to denote different entities. This error occurs when this practice is broken. Initially, we decided to mark all instances of the same entity in an input sentence. Because of this error, we could only mark the two instances involved in the examined relation. To explain this further, we cannot specify if two words in a sentence is the same entity solely based on its appearance, there would be a lot of problems that could happen and a lot more work to prevent those.\n",
        "```\n",
        "# VLSP2020_RE_train\\23351316.conll\n",
        "1-646\t2909-2913\tCông\t_\t_\t_\t_\t\n",
        "1-647\t2914-2916\tty\t_\t_\t_\t_\t\n",
        "1-648\t2917-2920\tcon\t_\t_\t_\t_\t\n",
        "1-649\t2921-2926\tWaymo\t*\tORGANIZATION\t_\t_\t\n",
        "1-650\t2927-2930\tcủa\t_\t_\t_\t_\t\n",
        "1-651\t2931-2939\tAlphabet\t*\tORGANIZATION\tPART – WHOLE\t1-649\t\n",
        "1-652\t2940-2943\tthì\t_\t_\t_\t_\t\n",
        "1-653\t2944-2948\tđang\t_\t_\t_\t_\t\n",
        "```\n",
        "\n",
        "Regarding this error, there are some cases really hard to handle that we decide to ignore, e.g.:\n",
        "```\n",
        "# VLSP2020_RE_train\\23351965.conll\n",
        "1-320\t1419-1427\tBrussels\t*\tLOCATION\tPART – WHOLE\t1-321\t\n",
        "1-321\t1428-1431\t(Bỉ\t*\tLOCATION\t_\t_\t\n",
        "1-322\t1432-1434\t).\t_\t_\t_\t_\t\n",
        "```\n",
        "\n",
        "In details, our algorithm detects the range of an entity by a contiguous sequence of rows having the same value in the `entity` column. In the case above, semantically, we as humans can detect that `Brussels` and `Bỉ` are different entities. However, we cannot know for sure if there are any cases that are not as clear as this. We also cannot solve it by only detect an entity by going down from the referenced index because of the fifth error. We call this **self-relation** error. We believe this error could be resolved in future work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "05LFe7LhCTci"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "format_code:\n",
        "\n",
        "0 for <entity1> <sep> <entity2> <sep> <sentence>\n",
        "where all instances of <entity1> and <entity2> are replaced by the corresponding tokens in <sentence>.\n",
        "\n",
        "1 for open tag and close tag for each entity are inserted into the sentence.\n",
        "\"\"\"\n",
        "class VlspPreprocessor:\n",
        "  special_token = {\n",
        "    # for format code 0\n",
        "    \"SEP\": \"<sep>\",\n",
        "    \"PERSON[1]\": \"<person1/>\",\n",
        "    \"PERSON[2]\": \"<person2/>\",\n",
        "    \"ORGANIZATION[1]\": \"<organization1/>\",\n",
        "    \"ORGANIZATION[2]\": \"<organization2/>\",\n",
        "    \"LOCATION[1]\": \"<location1/>\",\n",
        "    \"LOCATION[2]\": \"<location2/>\",\n",
        "    \n",
        "    # for format code 1\n",
        "    \"<PERSON>\": \"<person>\",\n",
        "    \"</PERSON>\": \"</person>\",\n",
        "    \"<ORGANIZATION>\": \"<organization>\",\n",
        "    \"</ORGANIZATION>\": \"</organization>\",\n",
        "    \"<LOCATION>\": \"<location>\",\n",
        "    \"</LOCATION>\": \"</location>\",\n",
        "  }\n",
        "  \n",
        "  eos_punctuation = [\".\", \"?\", \"!\"] # need updating end of sentence punctuations to be more legit\n",
        "  \n",
        "  def __init__(self, drop_no_relation_samples: bool=True, format_code: {0, 1}=0, run_vncorenlp_wseg: bool=False):\n",
        "    self.dataset = []\n",
        "    \n",
        "    self.label2id = {}\n",
        "    self.id2label = {}\n",
        "    self.drop_no_relation_samples = drop_no_relation_samples\n",
        "    \n",
        "    self.unformatted_offset = 0\n",
        "    self.format_code = format_code\n",
        "    self.run_vncorenlp_wseg = run_vncorenlp_wseg\n",
        "    \n",
        "    self.sentences = np.array([])\n",
        "    self.labels = np.array([], dtype=\"uint8\")\n",
        "    \n",
        "    self.self_relations = [] # for debugging error 7\n",
        "    \n",
        "  def __len__(self) -> int:\n",
        "    return len(self.sentences)\n",
        "  \n",
        "  def execute_all(self, src_dir: str, drop_no_relation_samples: bool=None, format_code: {0, 1}=None, run_vncorenlp_wseg: bool=None, shuffle: int=None):\n",
        "    print(f\"Executing {src_dir}...\")\n",
        "    self.load(src_dir, drop_no_relation_samples=drop_no_relation_samples)\n",
        "    print(\"Done loading.\")\n",
        "    self.format(format_code=format_code, run_vncorenlp_wseg=run_vncorenlp_wseg)\n",
        "    print(\"Done formatting.\")\n",
        "    if shuffle is not None:\n",
        "      self.shuffle(shuffle)\n",
        "      print(f\"Done shuffling with seed {shuffle}.\")\n",
        "    print(\"✅ Done all.\")\n",
        "    \n",
        "  def load(self, src_dir: str, drop_no_relation_samples: bool=None): # this function is designed to execute many times on many directories\n",
        "    if drop_no_relation_samples is not None:\n",
        "      self.drop_no_relation_samples = drop_no_relation_samples\n",
        "    \n",
        "    for root, _, files in os.walk(src_dir):\n",
        "      for file in files: # currently the structure is one tsv file per subfolder, but this loop is in case there are more\n",
        "        if os.path.join(\"VLSP2020_RE_train\", \"23352816.conll\") in root: # the tsv file in this subfolder is heavily corrupted, just ignore it for now\n",
        "          continue\n",
        "        if file.endswith(\".tsv\"):\n",
        "          self.root = root # for debugging\n",
        "          self.process_tsv(os.path.join(root, file))\n",
        "\n",
        "    if self.drop_no_relation_samples:\n",
        "      self._drop_no_relation_samples()\n",
        "    \n",
        "    self._build_id2label()\n",
        "  \n",
        "  def format(self, format_code: {0, 1}=None, run_vncorenlp_wseg: bool=None):\n",
        "    if format_code is not None:\n",
        "      self.format_code = format_code\n",
        "    if run_vncorenlp_wseg is not None:\n",
        "      self.run_vncorenlp_wseg = run_vncorenlp_wseg\n",
        "    \n",
        "    self._format(self.unformatted_offset)\n",
        "    if run_vncorenlp_wseg:\n",
        "      self._run_vncorenlp_wseg(self.unformatted_offset)\n",
        "    self.unformatted_offset = len(self.dataset)\n",
        "\n",
        "  def process_tsv(self, tsv_dir: str):\n",
        "    df = pd.read_csv(tsv_dir, sep=\"\\t\", comment=\"#\", quotechar=\"\\t\", header=None)\n",
        "    if len(df.columns) < 8: # relation columns are missing\n",
        "      return\n",
        "    df.columns = [\"ann_idx\", \"range\", \"word\", \"var\", \"entity\", \"relation\", \"rel_heads\", \"-\"] # because of the meaningless \\t at the end of each line, we need one more column \"-\"\n",
        "\n",
        "    self._handle_word_with_entity_subword(df)\n",
        "\n",
        "    dataset_offset = len(self.dataset)\n",
        "    self._extract_sentences(df)\n",
        "    # self._extract_entities(df, dataset_offset) # because of changes due to error 7, no need to keep track of entities anymore\n",
        "    self._extract_relations(df, dataset_offset)\n",
        "\n",
        "  def _handle_word_with_entity_subword(self, df):\n",
        "    error_indices = df[df[\"ann_idx\"].shift(-1).apply(lambda i: i is not None and \".1\" in str(i))][\"word\"].index\n",
        "    offset = 0\n",
        "    for idx in error_indices:\n",
        "      idx += offset\n",
        "      entity_word = df.iloc[idx + 1][\"word\"]\n",
        "      prefix, suffix = df.iloc[idx][\"word\"].split(entity_word, 1)\n",
        "\n",
        "      if idx > 0 and df.iloc[idx - 1][\"entity\"] == df.iloc[idx + 1][\"entity\"]:\n",
        "        df.loc[idx + 1, \"relation\"] = \"_\"\n",
        "        df.loc[idx + 1, \"rel_heads\"] = \"_\"\n",
        "\n",
        "      if suffix != \"\":\n",
        "        df.loc[idx + 1.5] = [\"_\", \"_\", suffix, \"_\", \"_\", \"_\", \"_\", np.nan]\n",
        "        offset += 1\n",
        "\n",
        "      if prefix != \"\":\n",
        "        df.loc[idx, \"word\"] = prefix\n",
        "      else:\n",
        "        df = df.drop(idx)\n",
        "        offset -= 1\n",
        "\n",
        "      df = df.sort_index().reset_index(drop=True)\n",
        "\n",
        "  def _extract_sentences(self, df):\n",
        "    sentence = []\n",
        "    for word in df[\"word\"].values:\n",
        "      word = str(word) #  in VLSP2020_RE_dev/23352623.conll, the word \"nan\" counts as a float ¯\\_(ツ)_/¯\n",
        "      sentence.append(word)\n",
        "      if word[-1] in VlspPreprocessor.eos_punctuation:\n",
        "        self.dataset.append({ \"word_list\": np.array(sentence) })\n",
        "        sentence = []\n",
        "    self.dataset.append({ \"word_list\": np.array(sentence) })\n",
        "\n",
        "  def _extract_entities(self, df, dataset_offset: int): # deprecated\n",
        "    offset = 0\n",
        "    sample_idx = dataset_offset\n",
        "    entity_df = df[df[\"entity\"] != \"_\"][\"entity\"]\n",
        "    for entity, idx in zip(entity_df.values, entity_df.index):\n",
        "      while idx >= offset + len(self.dataset[sample_idx][\"word_list\"]):\n",
        "        offset += len(self.dataset[sample_idx][\"word_list\"])\n",
        "        sample_idx += 1\n",
        "\n",
        "      if \"entities\" not in self.dataset[sample_idx]:\n",
        "        self.dataset[sample_idx][\"entities\"] = {} # set of entity variables\n",
        "\n",
        "      if entity not in self.dataset[sample_idx][\"entities\"]\\\n",
        "        and \"MISCELLANEOUS\" not in entity: # error 6, ignore miscellaneous entities\n",
        "        self.dataset[sample_idx][\"entities\"].update(entity)\n",
        "\n",
        "  def _extract_relations(self, df, dataset_offset: int):\n",
        "    offset = 0\n",
        "    sample_idx = dataset_offset\n",
        "    relation_df = df[df[\"relation\"] != \"_\"][[\"relation\", \"entity\", \"rel_heads\"]]    \n",
        "    for (relations, entity, rel_heads), idx in zip(relation_df.values, relation_df.index):\n",
        "      if \"MISCELLANEOUS\" in entity: # error 6, ignore miscellaneous entity\n",
        "        continue\n",
        "      \n",
        "      relations = relations.split(\"|\")\n",
        "      rel_heads = rel_heads.split(\"|\")\n",
        "        \n",
        "      while idx >= offset + len(self.dataset[sample_idx][\"word_list\"]):\n",
        "        offset += len(self.dataset[sample_idx][\"word_list\"])\n",
        "        sample_idx += 1\n",
        "      sample = self.dataset[sample_idx]\n",
        "      entity_range = self._find_range(df, idx, offset=offset)\n",
        "      entity = entity.split(\"[\")[0]\n",
        "      \n",
        "      for i in range(len(relations)):\n",
        "        if relations[i] == \"*\": # error 6, ignore miscellaneous relations in VLSP2020_RE_train\\23351515.conll and 23351856.conll\n",
        "          continue\n",
        "\n",
        "        other_entity_df = df[df[\"ann_idx\"] == rel_heads[i].split(\"[\")[0]][\"entity\"]\n",
        "        other_entity = other_entity_df.values[0].split(\"[\")[0]\n",
        "        other_entity_idx = other_entity_df.index[0]\n",
        "        \n",
        "        if \"MISCELLANEOUS\" in other_entity: # error 6, ignore miscellaneous entity\n",
        "          continue\n",
        "        if offset > other_entity_idx or other_entity_idx >= offset + len(sample[\"word_list\"]): # error 3, ignore inter-sentence relations\n",
        "          continue\n",
        "        \n",
        "        if \"relations\" not in sample:\n",
        "          sample[\"relations\"] = []\n",
        "        \n",
        "        other_entity_range = self._find_range(df, other_entity_idx, offset=offset)\n",
        "        \n",
        "        if entity_range[0] == other_entity_range[0] and  entity_range[1] == other_entity_range[1]: # error 7, ignore and save to debug log\n",
        "          self.self_relations.append((relations[i], rel_heads[i], self.root))\n",
        "          continue\n",
        "        \n",
        "        assert entity_range[1] <= other_entity_range[0] or other_entity_range[1] <= entity_range[0]\n",
        "        sample[\"relations\"].append((entity, entity_range, other_entity, other_entity_range, relations[i]))\n",
        "\n",
        "        if relations[i] not in self.label2id: # update label2id mapping\n",
        "          self.label2id[relations[i]] = len(self.label2id)\n",
        "          \n",
        "  def _find_range(self, df: pd.DataFrame, idx: int, offset: int=0) -> tuple[int, int]:\n",
        "    entity = df.iloc[int(idx)][\"entity\"]\n",
        "    x = int(idx)\n",
        "    while x > 0 and df.iloc[x - 1][\"entity\"] == entity:\n",
        "      x -= 1\n",
        "    y = int(idx) + 1\n",
        "    while y < df.shape[0] and df.iloc[y][\"entity\"] == entity:\n",
        "      y += 1\n",
        "    assert x - offset < y - offset\n",
        "    return (x - offset, y - offset)\n",
        "\n",
        "  def _drop_no_relation_samples(self, dataset_offset: int=0):\n",
        "    clean_dataset = []\n",
        "    for sample in self.dataset[dataset_offset:]:\n",
        "      if \"relations\" in sample:\n",
        "        clean_dataset.append(sample)\n",
        "    self.dataset = [*self.dataset[:dataset_offset], *clean_dataset]\n",
        "  \n",
        "  def _build_id2label(self):\n",
        "    self.id2label = {v: k for k, v in self.label2id.items()}\n",
        "  \n",
        "  def _format(self, dataset_offset: int):\n",
        "    for sample in self.dataset[dataset_offset:]:\n",
        "      if \"relations\" not in sample:\n",
        "        continue\n",
        "      \n",
        "      for ent1, ent1_range, ent2, ent2_range, rel_type in sample[\"relations\"]:\n",
        "        self.labels = np.append(self.labels, self.label2id[rel_type])\n",
        "        sentence = np.array([])\n",
        "        \n",
        "        if self.format_code == 0:\n",
        "          ent2 = f\"{ent2}[{2 if ent1 == ent2 else 1}]\"\n",
        "          ent1 = f\"{ent1}[1]\"\n",
        "          sentence = np.append(sentence, [\n",
        "            VlspPreprocessor.special_token[ent1],\n",
        "            VlspPreprocessor.special_token[\"SEP\"],\n",
        "            VlspPreprocessor.special_token[ent2],\n",
        "            VlspPreprocessor.special_token[\"SEP\"],\n",
        "          ])\n",
        "        \n",
        "        if ent1_range[0] >= ent2_range[1]:\n",
        "          ent1, ent1_range, ent2, ent2_range = ent2, ent2_range, ent1, ent1_range\n",
        "        assert ent1_range[1] <= ent2_range[0]\n",
        "        \n",
        "        if self.format_code == 0:\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][:ent1_range[0]])\n",
        "          sentence = np.append(sentence, [VlspPreprocessor.special_token[ent1]])\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][ent1_range[1]:ent2_range[0]])\n",
        "          sentence = np.append(sentence, [VlspPreprocessor.special_token[ent2]])\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][ent2_range[1]:])\n",
        "        elif self.format_code == 1:\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][:ent1_range[0]])\n",
        "          sentence = np.append(sentence, [VlspPreprocessor.special_token[f\"<{ent1}>\"]])\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][ent1_range[0]:ent1_range[1]])\n",
        "          sentence = np.append(sentence, [VlspPreprocessor.special_token[f\"</{ent1}>\"]])\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][ent1_range[1]:ent2_range[0]])\n",
        "          sentence = np.append(sentence, [VlspPreprocessor.special_token[f\"<{ent2}>\"]])\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][ent2_range[0]:ent2_range[1]])\n",
        "          sentence = np.append(sentence, [VlspPreprocessor.special_token[f\"</{ent2}>\"]])\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][ent2_range[1]:])\n",
        "        else:\n",
        "          raise ValueError(\"Preprocessor is using some non-predefined format code.\")\n",
        "        \n",
        "        self.sentences = np.append(self.sentences, \" \".join(sentence))\n",
        "        \n",
        "    assert len(self.sentences) == len(self.labels)\n",
        "\n",
        "  def _run_vncorenlp_wseg(self, dataset_offset: int=0):\n",
        "    def sentence_transform(s: str):\n",
        "      s = vncorenlp_model.word_segment(s)[0]\n",
        "      for _, token in VlspPreprocessor.special_token.items():\n",
        "        wseg_token = vncorenlp_model.word_segment(token)[0]\n",
        "        s = s.replace(wseg_token, token)\n",
        "      return s\n",
        "\n",
        "    array_transform = np.vectorize(sentence_transform)\n",
        "    self.sentences = np.concatenate([\n",
        "      self.sentences[:dataset_offset],\n",
        "      array_transform(self.sentences[dataset_offset:])\n",
        "    ])\n",
        "  \n",
        "  def shuffle(self, seed: int=42):\n",
        "    np.random.seed(seed)\n",
        "    mask = np.random.permutation(len(self.sentences))\n",
        "    assert len(self.sentences) == len(self.labels)\n",
        "    self.sentences = self.sentences[mask]\n",
        "    self.labels = self.labels[mask]\n",
        "  \n",
        "  def clear(self):\n",
        "    self.dataset.clear()\n",
        "    self.label2id.clear()\n",
        "    self.id2label.clear()\n",
        "    self.reset_format()\n",
        "    \n",
        "  def reset_format(self):\n",
        "    self.unformatted_offset = 0\n",
        "    self.sentences = np.array([])\n",
        "    self.labels = np.array([], dtype=\"uint8\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execution\n",
        "\n",
        "The original test set from VLSP does not provide relation labels. Therefore, we decided to use the given dev set as the test set and split the given train set into new train and dev sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We prepare a procedure to save data in a json file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def save_data(dir, dataset, labels, shuffle_seed):\n",
        "  data = {\n",
        "    \"dataset\": list(dataset),\n",
        "    \"labels\": list(labels),\n",
        "    \"num_samples\": len(dataset),\n",
        "    \"shuffle_seed\": shuffle_seed,\n",
        "  }\n",
        "  with open(dir, \"w\") as file:\n",
        "    json.dump(json.dumps(data), file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fLHwChL1UAlI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing VLSP2020\\VLSP2020_RE_train...\n",
            "Done loading.\n",
            "Done formatting.\n",
            "Done shuffling with seed 42.\n",
            "✅ Done all.\n"
          ]
        }
      ],
      "source": [
        "dir = os.path.join(\"VLSP2020\", \"VLSP2020_RE_train\")\n",
        "preprocessor = VlspPreprocessor()\n",
        "preprocessor.execute_all(dir, format_code=0, run_vncorenlp_wseg=True, shuffle=42) # start with format 0\n",
        "\n",
        "# save first 2000 samples as the train set and the remaining as the dev set\n",
        "train_set_format0 = preprocessor.sentences[:2000]\n",
        "train_labels = [int(x) for x in preprocessor.labels[:2000]]\n",
        "dev_set_format0 = preprocessor.sentences[2000:]\n",
        "dev_labels = [int(x) for x in preprocessor.labels[2000:]]\n",
        "\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format0\", \"train.json\"),\n",
        "  preprocessor.sentences[:2000],\n",
        "  [int(x) for x in preprocessor.labels[:2000]],\n",
        "  shuffle_seed=42\n",
        ")\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format0\", \"dev.json\"),\n",
        "  preprocessor.sentences[2000:],\n",
        "  [int(x) for x in preprocessor.labels[2000:]],\n",
        "  shuffle_seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reset format and run again with format 1\n",
        "preprocessor.reset_format()\n",
        "preprocessor.format(format_code=1, run_vncorenlp_wseg=True)\n",
        "preprocessor.shuffle(42)\n",
        "\n",
        "# save first 2000 samples as the train set and the remaining as the dev set\n",
        "train_set_format1 = preprocessor.sentences[:2000]\n",
        "dev_set_format1 = preprocessor.sentences[2000:]\n",
        "\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format1\", \"train.json\"),\n",
        "  preprocessor.sentences[:2000],\n",
        "  [int(x) for x in preprocessor.labels[:2000]],\n",
        "  shuffle_seed=42\n",
        ")\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format1\", \"dev.json\"),\n",
        "  preprocessor.sentences[2000:],\n",
        "  [int(x) for x in preprocessor.labels[2000:]],\n",
        "  shuffle_seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing VLSP2020\\VLSP2020_RE_dev...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done loading.\n",
            "Done formatting.\n",
            "Done shuffling with seed 42.\n",
            "✅ Done all.\n"
          ]
        }
      ],
      "source": [
        "# clear all data to run again with new data for the test set\n",
        "dir = os.path.join(\"VLSP2020\", \"VLSP2020_RE_dev\")\n",
        "preprocessor.clear()\n",
        "preprocessor.execute_all(dir, format_code=0, run_vncorenlp_wseg=True, shuffle=42)\n",
        "\n",
        "test_set_format0 = preprocessor.sentences\n",
        "test_labels = [int(x) for x in preprocessor.labels]\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format0\", \"test.json\"),\n",
        "  preprocessor.sentences,\n",
        "  [int(x) for x in preprocessor.labels],\n",
        "  shuffle_seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reset format and run again with format 1\n",
        "preprocessor.reset_format()\n",
        "preprocessor.format(format_code=1, run_vncorenlp_wseg=True)\n",
        "preprocessor.shuffle(42)\n",
        "\n",
        "test_set_format1 = preprocessor.sentences\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format1\", \"test.json\"),\n",
        "  preprocessor.sentences,\n",
        "  [int(x) for x in preprocessor.labels],\n",
        "  shuffle_seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The length of each dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 2000\n",
            "Number of development samples: 599\n",
            "Number of testing samples: 1437\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of training samples: {len(train_labels)}\")\n",
        "print(f\"Number of development samples: {len(dev_labels)}\")\n",
        "print(f\"Number of testing samples: {len(test_labels)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some of the samples in format 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['<location1/> <sep> <location2/> <sep> Họ & tên : Nguyễn_Thu_Hà_Sinh ngày : 18/12/1998 Quê_quán : Hà_Nội Chỗ ở hiện_tại : <location2/> , <location1/> Hiện là sinh_viên khoa Báo_chí & Truyền_thông , Trường ĐH Khoa_học_xã_hội & Nhân_văn Hà_Nội Chiều cao : 1m70 - Cân nặng : 50kg - Số_đo 3 vòng : 83-62-90 cm Sở_thích : Đọc sách , du_lịch Ước_mơ : Trở_thành nhà_báo .',\n",
              "       '<location1/> <sep> <location2/> <sep> Theo đó , tấm vé trúng giải được phát_hành lúc 16h49 ngày 21/9 tại một điểm bán hàng thuộc ấp Thạch_An , xã Trung_An , <location2/> , <location1/> .',\n",
              "       '<location1/> <sep> <organization1/> <sep> Liliane_Bettencourt là con duy_nhất của nhà hoá_học Eugene_Schueller , người đã sáng_lập tập_đoàn mỹ_phẩm danh_tiếng của <location1/> <organization1/> từ hồi đầu thế_kỷ 20 .',\n",
              "       '<location1/> <sep> <person1/> <sep> Triều_Tiên hôm_nay còn tuyên_bố có_thể thử bom nhiệt_hạch trên Thái_Bình_Dương , sau khi Tổng_thống <location1/> <person1/> doạ \" huỷ_diệt hoàn_toàn \" nước này .',\n",
              "       '<person1/> <sep> <person2/> <sep> Cảm_thông trước hoàn_cảnh của vợ_chồng anh <person1/> , chị <person2/> , sau khi đọc bài viết , nhiều độc_giả có tấm lòng hảo_tâm đã gửi tiền ủng_hộ gồm : Cty Cây giống Hương_Trà ( thị_xã Hương_Trà - TT Huế ) : 500.000 đồng ; Trường THCS Hà_Thế_Hạnh - Tứ_Hạ - TX Hương_Trà ( nơi cháu Phan_Thành_Lĩnh học lớp 6 ) : 1.000.000 đồng ; Chị Thuỳ_Tin ( TP .',\n",
              "       '<location1/> <sep> <person1/> <sep> Khi đó , trên đường có xe đầu kéo 77 C-143 . 82 kéo sơ_mi rơ moóc 77 R - 022.74 do anh <person1/> ( 38 tuổi , ngụ tổ 8 , phường Bùi_Thị_Xuân , TP. Quy_Nhơn , <location1/> ) điều_khiển đang đi theo hướng Bắc-Nam.',\n",
              "       '<organization1/> <sep> <person1/> <sep> Nữ_sinh ĐH Y mất_tích : Điểm tổng_kết trên 9,0 nhưng bỏ thi tốt_nghiệp không lý_do Theo giáo_viên chủ_nhiệm từ lúc đi thực_tập tại thành_phố Uông_Bí về , nữ_sinh Vũ_Thị_Diên có những tâm_lý bất_thường , bỏ thi tốt_nghiệp không rõ lý_do Liên_quan đến nữ_sinh <person1/> - sinh_viên lớp Kỹ_thuật y_học K5 , <organization1/> mất_tích , ngày 22/9 , giáo_viên chủ_nhiệm lớp đã có những chia_sẻ về Diên trong quá_trình học_tập tại trường .',\n",
              "       '<person1/> <sep> <person2/> <sep> Bây_giờ hai vợ_chồng cùng thi_đấu có_thể giúp nhau nhiều , nhất_là chúng_tôi cùng thương_tật , cùng nội_dung nên anh <person2/> có_thể chỉ thêm cho tôi rất nhiều \" , <person1/> chia_sẻ .',\n",
              "       '<location1/> <sep> <location2/> <sep> Đến tháng 3/2014 , thầy Tuệ được vào biên_chế và làm_việc tại trường Tiểu_học Thanh_Thuỷ <location2/> , <location1/> ) .',\n",
              "       '<organization1/> <sep> <person1/> <sep> Phút 68 , <person1/> suýt chút nữa khiến Tấn_Trường phải trả_giá từ một pha đá phạt_góc của <organization1/> .'],\n",
              "      dtype='<U3135')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev_set_format0[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some of the samples in format 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Họ & tên : Nguyễn_Thu_Hà_Sinh ngày : 18/12/1998 Quê_quán : Hà_Nội Chỗ ở hiện_tại : <location> Q.Đống Đa </location> , <location> Hà_Nội </location> Hiện là sinh_viên khoa Báo_chí & Truyền_thông , Trường ĐH Khoa_học_xã_hội & Nhân_văn Hà_Nội Chiều cao : 1m70 - Cân nặng : 50kg - Số_đo 3 vòng : 83-62-90 cm Sở_thích : Đọc sách , du_lịch Ước_mơ : Trở_thành nhà_báo .',\n",
              "       'Theo đó , tấm vé trúng giải được phát_hành lúc 16h49 ngày 21/9 tại một điểm bán hàng thuộc ấp Thạch_An , xã Trung_An , <location> huyện Củ_Chi </location> , <location> TP. HCM </location> .',\n",
              "       \"Liliane_Bettencourt là con duy_nhất của nhà hoá_học Eugene_Schueller , người đã sáng_lập tập_đoàn mỹ_phẩm danh_tiếng của <location> Pháp </location> <organization> L ' Oreal </organization> từ hồi đầu thế_kỷ 20 .\",\n",
              "       'Triều_Tiên hôm_nay còn tuyên_bố có_thể thử bom nhiệt_hạch trên Thái_Bình_Dương , sau khi Tổng_thống <location> Mỹ </location> <person> Donald_Trump </person> doạ \" huỷ_diệt hoàn_toàn \" nước này .',\n",
              "       'Cảm_thông trước hoàn_cảnh của vợ_chồng anh <person> Hà </person> , chị <person> Lơn </person> , sau khi đọc bài viết , nhiều độc_giả có tấm lòng hảo_tâm đã gửi tiền ủng_hộ gồm : Cty Cây giống Hương_Trà ( thị_xã Hương_Trà - TT Huế ) : 500.000 đồng ; Trường THCS Hà_Thế_Hạnh - Tứ_Hạ - TX Hương_Trà ( nơi cháu Phan_Thành_Lĩnh học lớp 6 ) : 1.000.000 đồng ; Chị Thuỳ_Tin ( TP .',\n",
              "       'Khi đó , trên đường có xe đầu kéo 77 C-143 . 82 kéo sơ_mi rơ moóc 77 R - 022.74 do anh <person> Hồ_Thanh_Lợi </person> ( 38 tuổi , ngụ tổ 8 , phường Bùi_Thị_Xuân , TP. Quy_Nhơn , <location> tỉnh Bình_Định </location> ) điều_khiển đang đi theo hướng Bắc-Nam.',\n",
              "       'Nữ_sinh ĐH Y mất_tích : Điểm tổng_kết trên 9,0 nhưng bỏ thi tốt_nghiệp không lý_do Theo giáo_viên chủ_nhiệm từ lúc đi thực_tập tại thành_phố Uông_Bí về , nữ_sinh Vũ_Thị_Diên có những tâm_lý bất_thường , bỏ thi tốt_nghiệp không rõ lý_do Liên_quan đến nữ_sinh <person> Vũ_Thị_Diên </person> - sinh_viên lớp Kỹ_thuật y_học K5 , <organization> Đại_học Y_dược Hải_Phòng </organization> mất_tích , ngày 22/9 , giáo_viên chủ_nhiệm lớp đã có những chia_sẻ về Diên trong quá_trình học_tập tại trường .',\n",
              "       'Bây_giờ hai vợ_chồng cùng thi_đấu có_thể giúp nhau nhiều , nhất_là chúng_tôi cùng thương_tật , cùng nội_dung nên anh <person> Hùng </person> có_thể chỉ thêm cho tôi rất nhiều \" , <person> Hải </person> chia_sẻ .',\n",
              "       'Đến tháng 3/2014 , thầy Tuệ được vào biên_chế và làm_việc tại trường Tiểu_học Thanh_Thuỷ <location> ( Lệ_Thuỷ </location> , <location> Quảng_Bình </location> ) .',\n",
              "       'Phút 68 , <person> Thế_Cường </person> suýt chút nữa khiến Tấn_Trường phải trả_giá từ một pha đá phạt_góc của <organization> SLNA </organization> .'],\n",
              "      dtype='<U3127')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev_set_format1[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Statistics by labels in train set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'AFFILIATION': 0, 'PART – WHOLE': 1, 'LOCATED': 2, 'PERSONAL - SOCIAL': 3}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th># samples</th>\n",
              "      <th>Ratio (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>900</td>\n",
              "      <td>45.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>556</td>\n",
              "      <td>27.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>469</td>\n",
              "      <td>23.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>75</td>\n",
              "      <td>3.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   # samples  Ratio (%)\n",
              "2        900      45.00\n",
              "0        556      27.80\n",
              "1        469      23.45\n",
              "3         75       3.75"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(preprocessor.label2id)\n",
        "df = pd.DataFrame(pd.Series(train_labels).value_counts(), columns=[\"# samples\"])\n",
        "df[\"Ratio (%)\"] = df[\"# samples\"] / df[\"# samples\"].sum() * 100\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Statistics by labels in dev set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'AFFILIATION': 0, 'PART – WHOLE': 1, 'LOCATED': 2, 'PERSONAL - SOCIAL': 3}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th># samples</th>\n",
              "      <th>Ratio (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>256</td>\n",
              "      <td>42.737896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>188</td>\n",
              "      <td>31.385643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>132</td>\n",
              "      <td>22.036728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23</td>\n",
              "      <td>3.839733</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   # samples  Ratio (%)\n",
              "2        256  42.737896\n",
              "0        188  31.385643\n",
              "1        132  22.036728\n",
              "3         23   3.839733"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(preprocessor.label2id)\n",
        "df = pd.DataFrame(pd.Series(dev_labels).value_counts(), columns=[\"# samples\"])\n",
        "df[\"Ratio (%)\"] = df[\"# samples\"] / df[\"# samples\"].sum() * 100\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Statistics by labels in test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'AFFILIATION': 0, 'PART – WHOLE': 1, 'LOCATED': 2, 'PERSONAL - SOCIAL': 3}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th># samples</th>\n",
              "      <th>Ratio (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>506</td>\n",
              "      <td>35.212248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>506</td>\n",
              "      <td>35.212248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>327</td>\n",
              "      <td>22.755741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>98</td>\n",
              "      <td>6.819763</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   # samples  Ratio (%)\n",
              "0        506  35.212248\n",
              "1        506  35.212248\n",
              "2        327  22.755741\n",
              "3         98   6.819763"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(preprocessor.label2id)\n",
        "df = pd.DataFrame(pd.Series(test_labels).value_counts(), columns=[\"# samples\"])\n",
        "df[\"Ratio (%)\"] = df[\"# samples\"] / df[\"# samples\"].sum() * 100\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All self-relation errors the preprocessor detected:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('PART – WHOLE', '1-321', 'VLSP2020\\\\VLSP2020_RE_train\\\\23351965.conll'),\n",
              " ('PART – WHOLE', '1-161', 'VLSP2020\\\\VLSP2020_RE_train\\\\23352701.conll'),\n",
              " ('PART – WHOLE', '1-885', 'VLSP2020\\\\VLSP2020_RE_train\\\\23352753.conll'),\n",
              " ('PART – WHOLE', '1-41', 'VLSP2020\\\\VLSP2020_RE_train\\\\23353786.conll'),\n",
              " ('PART – WHOLE', '1-390', 'VLSP2020\\\\VLSP2020_RE_train\\\\23353891.conll'),\n",
              " ('PART – WHOLE', '1-350', 'VLSP2020\\\\VLSP2020_RE_train\\\\23354619.conll'),\n",
              " ('PART – WHOLE', '1-423', 'VLSP2020\\\\VLSP2020_RE_train\\\\23354619.conll'),\n",
              " ('PART – WHOLE', '1-1375', 'VLSP2020\\\\VLSP2020_RE_train\\\\23354880.conll'),\n",
              " ('PART – WHOLE', '1-151', 'VLSP2020\\\\VLSP2020_RE_dev\\\\23352337.conll'),\n",
              " ('PART – WHOLE', '1-393', 'VLSP2020\\\\VLSP2020_RE_dev\\\\23352396.conll'),\n",
              " ('PART – WHOLE', '1-197', 'VLSP2020\\\\VLSP2020_RE_dev\\\\23352445.conll'),\n",
              " ('PART – WHOLE', '1-741', 'VLSP2020\\\\VLSP2020_RE_dev\\\\23352491.conll'),\n",
              " ('AFFILIATION', '1-208', 'VLSP2020\\\\VLSP2020_RE_dev\\\\23352585.conll')]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocessor.self_relations"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
