{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzmyfWAy0pJw"
      },
      "source": [
        "# VLSP Dataset Preparation\n",
        "\n",
        "This notebook is a walkthrough of our preprocessing step for the VLSP2020 Relation Extraction dataset (view more [here](https://vlsp.org.vn/vlsp2020/eval/re)). Preprocessing this dataset is quite a challenge due to many points that need fixing in the annotation. Additionally, to meet the best setting for the experiment, we propose two different input formats. At the end of this step, a folder named `vlsp_preprocessed` is created with two subfolders for two input formats, each containing three datasets (train, dev, and test) as json files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing Environment\n",
        "\n",
        "PhoBERT requires Vietnamese sentences to be word-segmented by VnCoreNLP in advanced. Therefore, we need to set up the model and load it for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4qltZcTHBKS",
        "outputId": "081cc2c9-ad87-4d25-a4bc-d9ec3da9fba7"
      },
      "outputs": [],
      "source": [
        "import py_vncorenlp\n",
        "import os\n",
        "\n",
        "vncorenlp_dir = \"D:/Projects/albert-imdb/vncorenlp\"\n",
        "project_dir = \"D:/Projects/albert-imdb\"\n",
        "\n",
        "# uncomment this line if VnCoreNLP has not been downloaded\n",
        "# py_vncorenlp.download_model(save_dir=vncorenlp_path)\n",
        "\n",
        "# load VnCoreNLP\n",
        "vncorenlp_model = py_vncorenlp.VnCoreNLP(save_dir=vncorenlp_dir, annotators=[\"wseg\"])\n",
        "\n",
        "# change directory back to the project\n",
        "os.chdir(project_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessor Implementation\n",
        "\n",
        "The whole process includes two main stages:\n",
        "\n",
        "1. **Loading stage**: The preprocessor loads all tsv files in a directory and processes each at a time as a dataframe. For convenience, we rename the columns as `ann_idx`, `range`, `word`, `var`, `entity`, `relation`, and `rel_heads`. Next, we extract sentences denoting roughly by a word ending with `.`, `?`, or `!`. Finally, for each relation detected in the dataframe, we save it with the entities and their ranges in attachment of the corresponding sentence.\n",
        "\n",
        "2. **Formatting stage**: For each relation, the preprocessor adds one more sample to `preprocessor.sentences`. The sentence will be formatted based on the format code provided (view our report for more details). If the `run_vncorenlp_wseg` is on, the sentence will continue to be segmented. For the special tokens inserted into the sentence beforehand, we simply substitute the segmented tokens with the original ones.\n",
        "\n",
        "After executing all, there is also an optional stage of shuffling the model-ready samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 7 annotation errors (some might not actually be errors but here we use \"error\" as a general term for all mentioned circumstances) that needs fixing:\n",
        "\n",
        "1. **Entity as a subword**: Normally, the annotation splits each word into each row by space. However, when a word is followed immediately by a punctuation and that word is a part of an entity, the annotator will duplicate that word into the next row. This sometimes happens with a \"word\" actually containing multiple words and the entity word is a part of that.\n",
        "```\n",
        "# VLSP2020_RE_train\\23357000.conll\n",
        "1-173\t807-810\tvăn\t*[8]\tORGANIZATION[8]\tAFFILIATION\t1-166[7_8]\n",
        "1-174\t811-816\tphòng\t*[8]\tORGANIZATION[8]\t_\t_\n",
        "1-175\t817-821\tkiến\t*[8]\tORGANIZATION[8]\t_\t_\n",
        "1-176\t822-826\ttrúc\t*[8]\tORGANIZATION[8]\t_\t_\n",
        "1-177\t827-833\t1+1>2;\t_\t_\t_\t_\n",
        "1-177.1\t827-832\t1+1>2\t*[8]\tORGANIZATION[8]\tAFFILIATION\t1-166[7_8]\n",
        "```\n",
        "\n",
        "2. **Not separated by space**: There are some cases that the annotator left two words or more with spaces in between in a row and an entity part is in that chunk (normally at the end or beginning). In order to make the data meaningful, we have to separate that entity part out. However, the difficult part is that we cannot detect which part belongs to an entity. Until the time writing this, we have only examined one file with this error. However, that file is also heavily corrupted with many other problems so we decided to ignore the whole file. Note that there are cases similar in structure but the entity part is duplicated into the next row. Those cases count as the first error.\n",
        "```\n",
        "# VLSP2020_RE_train\\23352816.conll\n",
        "1-153\t756-763\t. VMISS\t*\tORGANIZATION\t_\t_\t_\n",
        "```\n",
        "\n",
        "3. **Inter-sentence relations**: The task is intra-sentence but there are some relations between entities belong to different sentences. Therefore, we have to get rid of those relations. This is addressed by checking if the index of the pointed entity is in the same sentence with the origin entity.\n",
        "\n",
        "4. **Relation annotation not only in the first row of the entity**: If an entity is annotated with a relation, that relation should be inserted to the first row of that entity. But there are some cases, such as when getting the entity with punctuation error (see the example in that case), the entity annotation is interrupted and then continue with reinserting the relation.\n",
        "\n",
        "5. **Relation not linking to the first word of the entity**: A relation should be linked to the first word of the other entity but when the other entity annotation is interrupted by the first error, the relation is linked to the row of the duplicated word with no punctuation. Due to this error, when finding the range of the entities, we have to go up and down the dataframe from the referenced index to check it.\n",
        "```\n",
        "# VLSP2020_RE_train\\23356574.conll\n",
        "1-742\t3340-3346\tchuyên\t*[18]\tORGANIZATION[18]\t_\t_\n",
        "1-743\t3347-3354\tnghiệp,\t_\t_\t_\t_\n",
        "1-743.1\t3347-3353\tnghiệp\t*[18]\tORGANIZATION[18]\tPART – WHOLE\t1-733[17_18]\n",
        "1-744\t3355-3357\tSở\t*[19]\tORGANIZATION[19]\tAFFILIATION|PART – WHOLE\t1-733[17_19]|1-743.1[18_19]\n",
        "1-745\t3358-3363\tGD-ĐT\t*[19]\tORGANIZATION[19]\t_\t_\n",
        "```\n",
        "\n",
        "6. **Miscellaneous entities and relations**: Some places in a dataframe, the annotation only detect there is some entity or relation but not specify which that is. For convenience, we ignore all miscellaneous entities and relations, as well as relations involving miscellaneous entities.\n",
        "```\n",
        "# VLSP2020_RE_train\\23366765.conll\n",
        "1-6\t30-35\tApple\t*[1]\tMISCELLANEOUS[1]\t\n",
        "1-7\t36-39\tPay\t*[1]\tMISCELLANEOUS[1]\t\n",
        "# VLSP2020_RE_train\\23351515.conll\n",
        "1-318\t1372-1376\ttỉnh\t*[19]\tLOCATION[19]\tPART – WHOLE|LOCATED|PART – WHOLE|*\t1-315[18_19]|1-301[15_19]|1-307[16_19]|1-310[17_19]\t\n",
        "1-319\t1377-1381\tBình\t*[19]\tLOCATION[19]\t_\t_\t\n",
        "1-320\t1382-1386\tĐịnh\t*[19]\tLOCATION[19]\t_\t_\t\n",
        "```\n",
        "\n",
        "7. **Same name for different entities**: Normally, if there are two or more entities with the same type, there will be an index after the entity type to denote different entities. This error occurs when this practice is broken. Initially, we decided to mark all instances of the same entity in an input sentence. Because of this error, we could only mark the two instances involved in the examined relation. To explain this further, we cannot specify if two words in a sentence is the same entity solely based on its appearance, there would be a lot of problems that could happen and a lot more work to prevent those.\n",
        "```\n",
        "# VLSP2020_RE_train\\23351316.conll\n",
        "1-646\t2909-2913\tCông\t_\t_\t_\t_\t\n",
        "1-647\t2914-2916\tty\t_\t_\t_\t_\t\n",
        "1-648\t2917-2920\tcon\t_\t_\t_\t_\t\n",
        "1-649\t2921-2926\tWaymo\t*\tORGANIZATION\t_\t_\t\n",
        "1-650\t2927-2930\tcủa\t_\t_\t_\t_\t\n",
        "1-651\t2931-2939\tAlphabet\t*\tORGANIZATION\tPART – WHOLE\t1-649\t\n",
        "1-652\t2940-2943\tthì\t_\t_\t_\t_\t\n",
        "1-653\t2944-2948\tđang\t_\t_\t_\t_\t\n",
        "```\n",
        "\n",
        "Regarding this error, there are some cases really hard to handle that we decide to ignore, e.g.:\n",
        "```\n",
        "# VLSP2020_RE_train\\23351965.conll\n",
        "1-320\t1419-1427\tBrussels\t*\tLOCATION\tPART – WHOLE\t1-321\t\n",
        "1-321\t1428-1431\t(Bỉ\t*\tLOCATION\t_\t_\t\n",
        "1-322\t1432-1434\t).\t_\t_\t_\t_\t\n",
        "```\n",
        "\n",
        "In details, our algorithm detects the range of an entity by a contiguous sequence of rows having the same value in the `entity` column. In the case above, semantically, we as humans can detect that `Brussels` and `Bỉ` are different entities. However, we cannot know for sure if there are any cases that are not as clear as this. We also cannot solve it by only detect an entity by going down from the referenced index because of the fifth error. We call this **self-relation** error. We believe this error could be resolved in future work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "05LFe7LhCTci"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "format_code:\n",
        "\n",
        "0 for <entity1> <sep> <entity2> <sep> <sentence>\n",
        "where all instances of <entity1> and <entity2> are replaced by the corresponding tokens in <sentence>.\n",
        "\n",
        "1 for open tag and close tag for each entity are inserted into the sentence.\n",
        "\"\"\"\n",
        "class VlspPreprocessor:\n",
        "  special_token = {\n",
        "    # for format code 0\n",
        "    \"SEP\": \"<sep>\",\n",
        "    \"PERSON[1]\": \"<person1/>\",\n",
        "    \"PERSON[2]\": \"<person2/>\",\n",
        "    \"ORGANIZATION[1]\": \"<organization1/>\",\n",
        "    \"ORGANIZATION[2]\": \"<organization2/>\",\n",
        "    \"LOCATION[1]\": \"<location1/>\",\n",
        "    \"LOCATION[2]\": \"<location2/>\",\n",
        "    \n",
        "    # for format code 1\n",
        "    \"<PERSON>\": \"<person>\",\n",
        "    \"</PERSON>\": \"</person>\",\n",
        "    \"<ORGANIZATION>\": \"<organization>\",\n",
        "    \"</ORGANIZATION>\": \"</organization>\",\n",
        "    \"<LOCATION>\": \"<location>\",\n",
        "    \"</LOCATION>\": \"</location>\",\n",
        "  }\n",
        "  \n",
        "  eos_punctuation = [\".\", \"?\", \"!\"] # need updating end of sentence punctuations to be more legit\n",
        "  \n",
        "  def __init__(self, drop_no_relation_samples: bool=True, format_code: {0, 1}=0, run_vncorenlp_wseg: bool=False):\n",
        "    self.dataset = []\n",
        "    \n",
        "    self.label2id = {}\n",
        "    self.id2label = {}\n",
        "    self.drop_no_relation_samples = drop_no_relation_samples\n",
        "    \n",
        "    self.unformatted_offset = 0\n",
        "    self.format_code = format_code\n",
        "    self.run_vncorenlp_wseg = run_vncorenlp_wseg\n",
        "    \n",
        "    self.sentences = np.array([])\n",
        "    self.labels = np.array([], dtype=\"uint8\")\n",
        "    \n",
        "    self.self_relations = [] # for debugging error 7\n",
        "    \n",
        "  def __len__(self) -> int:\n",
        "    return len(self.sentences)\n",
        "  \n",
        "  def execute_all(self, src_dir: str, drop_no_relation_samples: bool=None, format_code: {0, 1}=None, run_vncorenlp_wseg: bool=None, shuffle: int=None):\n",
        "    print(f\"Executing {src_dir}...\")\n",
        "    self.load(src_dir, drop_no_relation_samples=drop_no_relation_samples)\n",
        "    print(\"Done loading.\")\n",
        "    self.format(format_code=format_code, run_vncorenlp_wseg=run_vncorenlp_wseg)\n",
        "    print(\"Done formatting.\")\n",
        "    if shuffle is not None:\n",
        "      self.shuffle(shuffle)\n",
        "      print(f\"Done shuffling with seed {shuffle}.\")\n",
        "    print(\"✅ Done all.\")\n",
        "    \n",
        "  def load(self, src_dir: str, drop_no_relation_samples: bool=None): # this function is designed to execute many times on many directories\n",
        "    if drop_no_relation_samples is not None:\n",
        "      self.drop_no_relation_samples = drop_no_relation_samples\n",
        "    \n",
        "    for root, _, files in os.walk(src_dir):\n",
        "      for file in files: # currently the structure is one tsv file per subfolder, but this loop is in case there are more\n",
        "        if os.path.join(\"VLSP2020_RE_train\", \"23352816.conll\") in root: # the tsv file in this subfolder is heavily corrupted, just ignore it for now\n",
        "          continue\n",
        "        if file.endswith(\".tsv\"):\n",
        "          self.root = root # for debugging\n",
        "          self.process_tsv(os.path.join(root, file))\n",
        "\n",
        "    if self.drop_no_relation_samples:\n",
        "      self._drop_no_relation_samples()\n",
        "    \n",
        "    self._build_id2label()\n",
        "  \n",
        "  def format(self, format_code: {0, 1}=None, run_vncorenlp_wseg: bool=None):\n",
        "    if format_code is not None:\n",
        "      self.format_code = format_code\n",
        "    if run_vncorenlp_wseg is not None:\n",
        "      self.run_vncorenlp_wseg = run_vncorenlp_wseg\n",
        "    \n",
        "    self._format(self.unformatted_offset)\n",
        "    if run_vncorenlp_wseg:\n",
        "      self._run_vncorenlp_wseg(self.unformatted_offset)\n",
        "    self.unformatted_offset = len(self.dataset)\n",
        "\n",
        "  def process_tsv(self, tsv_dir: str):\n",
        "    df = pd.read_csv(tsv_dir, sep=\"\\t\", comment=\"#\", quotechar=\"\\t\", header=None)\n",
        "    if len(df.columns) < 8: # relation columns are missing\n",
        "      return\n",
        "    df.columns = [\"ann_idx\", \"range\", \"word\", \"var\", \"entity\", \"relation\", \"rel_heads\", \"-\"] # because of the meaningless \\t at the end of each line, we need one more column \"-\"\n",
        "\n",
        "    self._handle_word_with_entity_subword(df)\n",
        "\n",
        "    dataset_offset = len(self.dataset)\n",
        "    self._extract_sentences(df)\n",
        "    # self._extract_entities(df, dataset_offset) # because of changes due to error 7, no need to keep track of entities anymore\n",
        "    self._extract_relations(df, dataset_offset)\n",
        "\n",
        "  def _handle_word_with_entity_subword(self, df):\n",
        "    error_indices = df[df[\"ann_idx\"].shift(-1).apply(lambda i: i is not None and \".1\" in str(i))][\"word\"].index\n",
        "    offset = 0\n",
        "    for idx in error_indices:\n",
        "      idx += offset\n",
        "      entity_word = df.iloc[idx + 1][\"word\"]\n",
        "      prefix, suffix = df.iloc[idx][\"word\"].split(entity_word, 1)\n",
        "\n",
        "      if idx > 0 and df.iloc[idx - 1][\"entity\"] == df.iloc[idx + 1][\"entity\"]:\n",
        "        df.loc[idx + 1, \"relation\"] = \"_\"\n",
        "        df.loc[idx + 1, \"rel_heads\"] = \"_\"\n",
        "\n",
        "      if suffix != \"\":\n",
        "        df.loc[idx + 1.5] = [\"_\", \"_\", suffix, \"_\", \"_\", \"_\", \"_\", np.nan]\n",
        "        offset += 1\n",
        "\n",
        "      if prefix != \"\":\n",
        "        df.loc[idx, \"word\"] = prefix\n",
        "      else:\n",
        "        df = df.drop(idx)\n",
        "        offset -= 1\n",
        "\n",
        "      df = df.sort_index().reset_index(drop=True)\n",
        "\n",
        "  def _extract_sentences(self, df):\n",
        "    sentence = []\n",
        "    for word in df[\"word\"].values:\n",
        "      word = str(word) #  in VLSP2020_RE_dev/23352623.conll, the word \"nan\" counts as a float ¯\\_(ツ)_/¯\n",
        "      sentence.append(word)\n",
        "      if word[-1] in VlspPreprocessor.eos_punctuation:\n",
        "        self.dataset.append({ \"word_list\": np.array(sentence) })\n",
        "        sentence = []\n",
        "    self.dataset.append({ \"word_list\": np.array(sentence) })\n",
        "\n",
        "  def _extract_entities(self, df, dataset_offset: int): # deprecated\n",
        "    offset = 0\n",
        "    sample_idx = dataset_offset\n",
        "    entity_df = df[df[\"entity\"] != \"_\"][\"entity\"]\n",
        "    for entity, idx in zip(entity_df.values, entity_df.index):\n",
        "      while idx >= offset + len(self.dataset[sample_idx][\"word_list\"]):\n",
        "        offset += len(self.dataset[sample_idx][\"word_list\"])\n",
        "        sample_idx += 1\n",
        "\n",
        "      if \"entities\" not in self.dataset[sample_idx]:\n",
        "        self.dataset[sample_idx][\"entities\"] = {} # set of entity variables\n",
        "\n",
        "      if entity not in self.dataset[sample_idx][\"entities\"]\\\n",
        "        and \"MISCELLANEOUS\" not in entity: # error 6, ignore miscellaneous entities\n",
        "        self.dataset[sample_idx][\"entities\"].update(entity)\n",
        "\n",
        "  def _extract_relations(self, df, dataset_offset: int):\n",
        "    offset = 0\n",
        "    sample_idx = dataset_offset\n",
        "    relation_df = df[df[\"relation\"] != \"_\"][[\"relation\", \"entity\", \"rel_heads\"]]    \n",
        "    for (relations, entity, rel_heads), idx in zip(relation_df.values, relation_df.index):\n",
        "      if \"MISCELLANEOUS\" in entity: # error 6, ignore miscellaneous entity\n",
        "        continue\n",
        "      \n",
        "      relations = relations.split(\"|\")\n",
        "      rel_heads = rel_heads.split(\"|\")\n",
        "        \n",
        "      while idx >= offset + len(self.dataset[sample_idx][\"word_list\"]):\n",
        "        offset += len(self.dataset[sample_idx][\"word_list\"])\n",
        "        sample_idx += 1\n",
        "      sample = self.dataset[sample_idx]\n",
        "      entity_range = self._find_range(df, idx, offset=offset)\n",
        "      entity = entity.split(\"[\")[0]\n",
        "      \n",
        "      for i in range(len(relations)):\n",
        "        if relations[i] == \"*\": # error 6, ignore miscellaneous relations in VLSP2020_RE_train\\23351515.conll and 23351856.conll\n",
        "          continue\n",
        "\n",
        "        other_entity_df = df[df[\"ann_idx\"] == rel_heads[i].split(\"[\")[0]][\"entity\"]\n",
        "        other_entity = other_entity_df.values[0].split(\"[\")[0]\n",
        "        other_entity_idx = other_entity_df.index[0]\n",
        "        \n",
        "        if \"MISCELLANEOUS\" in other_entity: # error 6, ignore miscellaneous entity\n",
        "          continue\n",
        "        if offset > other_entity_idx or other_entity_idx >= offset + len(sample[\"word_list\"]): # error 3, ignore inter-sentence relations\n",
        "          continue\n",
        "        \n",
        "        if \"relations\" not in sample:\n",
        "          sample[\"relations\"] = []\n",
        "        \n",
        "        other_entity_range = self._find_range(df, other_entity_idx, offset=offset)\n",
        "        \n",
        "        if entity_range[0] == other_entity_range[0] and  entity_range[1] == other_entity_range[1]: # error 7, ignore and save to debug log\n",
        "          self.self_relations.append((relations[i], rel_heads[i], self.root))\n",
        "          continue\n",
        "        \n",
        "        assert entity_range[1] <= other_entity_range[0] or other_entity_range[1] <= entity_range[0]\n",
        "        sample[\"relations\"].append((entity, entity_range, other_entity, other_entity_range, relations[i]))\n",
        "\n",
        "        if relations[i] not in self.label2id: # update label2id mapping\n",
        "          self.label2id[relations[i]] = len(self.label2id)\n",
        "          \n",
        "  def _find_range(self, df: pd.DataFrame, idx: int, offset: int=0) -> tuple[int, int]:\n",
        "    entity = df.iloc[int(idx)][\"entity\"]\n",
        "    x = int(idx)\n",
        "    while x > 0 and df.iloc[x - 1][\"entity\"] == entity:\n",
        "      x -= 1\n",
        "    y = int(idx) + 1\n",
        "    while y < df.shape[0] and df.iloc[y][\"entity\"] == entity:\n",
        "      y += 1\n",
        "    assert x - offset < y - offset\n",
        "    return (x - offset, y - offset)\n",
        "\n",
        "  def _drop_no_relation_samples(self, dataset_offset: int=0):\n",
        "    clean_dataset = []\n",
        "    for sample in self.dataset[dataset_offset:]:\n",
        "      if \"relations\" in sample:\n",
        "        clean_dataset.append(sample)\n",
        "    self.dataset = [*self.dataset[:dataset_offset], *clean_dataset]\n",
        "  \n",
        "  def _build_id2label(self):\n",
        "    self.id2label = {v: k for k, v in self.label2id.items()}\n",
        "  \n",
        "  def _format(self, dataset_offset: int):\n",
        "    for sample in self.dataset[dataset_offset:]:\n",
        "      if \"relations\" not in sample:\n",
        "        continue\n",
        "      \n",
        "      for ent1, ent1_range, ent2, ent2_range, rel_type in sample[\"relations\"]:\n",
        "        self.labels = np.append(self.labels, self.label2id[rel_type])\n",
        "        sentence = np.array([])\n",
        "        \n",
        "        if self.format_code == 0:\n",
        "          ent2 = f\"{ent2}[{2 if ent1 == ent2 else 1}]\"\n",
        "          ent1 = f\"{ent1}[1]\"\n",
        "          sentence = np.append(sentence, [\n",
        "            VlspPreprocessor.special_token[ent1],\n",
        "            VlspPreprocessor.special_token[\"SEP\"],\n",
        "            VlspPreprocessor.special_token[ent2],\n",
        "            VlspPreprocessor.special_token[\"SEP\"],\n",
        "          ])\n",
        "        \n",
        "        if ent1_range[0] >= ent2_range[1]:\n",
        "          ent1, ent1_range, ent2, ent2_range = ent2, ent2_range, ent1, ent1_range\n",
        "        assert ent1_range[1] <= ent2_range[0]\n",
        "        \n",
        "        if self.format_code == 0:\n",
        "          sentence = np.concatenate([\n",
        "            sentence,\n",
        "            sample[\"word_list\"][:ent1_range[0]],\n",
        "            [VlspPreprocessor.special_token[ent1]],\n",
        "            sample[\"word_list\"][ent1_range[1]:ent2_range[0]],\n",
        "            [VlspPreprocessor.special_token[ent2]],\n",
        "            sample[\"word_list\"][ent2_range[1]:]\n",
        "          ])\n",
        "        elif self.format_code == 1:\n",
        "          sentence = np.concatenate([\n",
        "            sample[\"word_list\"][:ent1_range[0]],\n",
        "            [VlspPreprocessor.special_token[f\"<{ent1}>\"]],\n",
        "            sample[\"word_list\"][ent1_range[0]:ent1_range[1]],\n",
        "            [VlspPreprocessor.special_token[f\"</{ent1}>\"]],\n",
        "            sample[\"word_list\"][ent1_range[1]:ent2_range[0]],\n",
        "            [VlspPreprocessor.special_token[f\"<{ent2}>\"]],\n",
        "            sample[\"word_list\"][ent2_range[0]:ent2_range[1]],\n",
        "            [VlspPreprocessor.special_token[f\"</{ent2}>\"]],\n",
        "            sample[\"word_list\"][ent2_range[1]:]\n",
        "          ])\n",
        "        else:\n",
        "          raise ValueError(\"Preprocessor is using some non-predefined format code.\")\n",
        "        \n",
        "        self.sentences = np.append(self.sentences, \" \".join(sentence))\n",
        "        \n",
        "    assert len(self.sentences) == len(self.labels)\n",
        "\n",
        "  def _run_vncorenlp_wseg(self, dataset_offset: int=0):\n",
        "    def sentence_transform(s: str):\n",
        "      s = vncorenlp_model.word_segment(s)[0]\n",
        "      for _, token in VlspPreprocessor.special_token.items():\n",
        "        wseg_token = vncorenlp_model.word_segment(token)[0]\n",
        "        s = s.replace(wseg_token, token)\n",
        "      return s\n",
        "\n",
        "    array_transform = np.vectorize(sentence_transform)\n",
        "    self.sentences = np.concatenate([\n",
        "      self.sentences[:dataset_offset],\n",
        "      array_transform(self.sentences[dataset_offset:])\n",
        "    ])\n",
        "  \n",
        "  def shuffle(self, seed: int=42):\n",
        "    np.random.seed(seed)\n",
        "    mask = np.random.permutation(len(self.sentences))\n",
        "    assert len(self.sentences) == len(self.labels)\n",
        "    self.sentences = self.sentences[mask]\n",
        "    self.labels = self.labels[mask]\n",
        "  \n",
        "  def clear(self):\n",
        "    self.dataset.clear()\n",
        "    self.label2id.clear()\n",
        "    self.id2label.clear()\n",
        "    self.reset_format()\n",
        "    \n",
        "  def reset_format(self):\n",
        "    self.unformatted_offset = 0\n",
        "    self.sentences = np.array([])\n",
        "    self.labels = np.array([], dtype=\"uint8\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execution\n",
        "\n",
        "The original test set from VLSP does not provide relation labels. Therefore, we decided to use the given dev set as the test set and split the given train set into new train and dev sets. The seed used for shuffling is `42` in all cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We prepare a procedure to save data in a `jsonl` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "\n",
        "def save_data(dir, sentences, labels):\n",
        "  with jsonlines.open(dir, mode=\"w\") as writer:\n",
        "    writer.write_all([{ \"sentence\": x, \"label\": int(y) } for x, y in zip(sentences, labels)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load all documents in the provided training and development folder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessor = VlspPreprocessor()\n",
        "preprocessor.load(os.path.join(\"VLSP2020\", \"VLSP2020_RE_train\"))\n",
        "preprocessor.load(os.path.join(\"VLSP2020\", \"VLSP2020_RE_dev\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run and save datasets without VnCoreNLP word segmentation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fLHwChL1UAlI"
      },
      "outputs": [],
      "source": [
        "preprocessor.format(format_code=0, run_vncorenlp_wseg=False)\n",
        "preprocessor.shuffle(42)\n",
        "\n",
        "n = len(preprocessor.sentences)\n",
        "train_set_format0 = preprocessor.sentences[:int(.8 * n)]\n",
        "dev_set_format0 = preprocessor.sentences[int(.8 * n):int(.9 * n)]\n",
        "test_set_format0 = preprocessor.sentences[int(.9 * n):]\n",
        "\n",
        "train_labels = preprocessor.labels[:int(.8 * n)]\n",
        "dev_labels = preprocessor.labels[int(.8 * n):int(.9 * n)]\n",
        "test_labels = preprocessor.labels[int(.9 * n):]\n",
        "\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format0_nowseg\", \"train.jsonl\"),\n",
        "  train_set_format0, train_labels\n",
        ")\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format0_nowseg\", \"dev.jsonl\"),\n",
        "  dev_set_format0, dev_labels\n",
        ")\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format0_nowseg\", \"test.jsonl\"),\n",
        "  test_set_format0, test_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessor.reset_format()\n",
        "preprocessor.format(format_code=1, run_vncorenlp_wseg=False)\n",
        "preprocessor.shuffle(42)\n",
        "\n",
        "train_set_format1 = preprocessor.sentences[:int(.8 * n)]\n",
        "dev_set_format1 = preprocessor.sentences[int(.8 * n):int(.9 * n)]\n",
        "test_set_format1 = preprocessor.sentences[int(.9 * n):]\n",
        "\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format1_nowseg\", \"train.jsonl\"),\n",
        "  train_set_format1, train_labels\n",
        ")\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format1_nowseg\", \"dev.jsonl\"),\n",
        "  dev_set_format1, dev_labels\n",
        ")\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format1_nowseg\", \"test.jsonl\"),\n",
        "  test_set_format1, test_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run and save datasets with VnCoreNLP word segmentation enabled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessor.reset_format()\n",
        "preprocessor.format(format_code=0, run_vncorenlp_wseg=True)\n",
        "preprocessor.shuffle(42)\n",
        "\n",
        "train_set_format0 = preprocessor.sentences[:int(.8 * n)]\n",
        "dev_set_format0 = preprocessor.sentences[int(.8 * n):int(.9 * n)]\n",
        "test_set_format0 = preprocessor.sentences[int(.9 * n):]\n",
        "\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format0\", \"train.jsonl\"),\n",
        "  train_set_format0, train_labels\n",
        ")\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format0\", \"dev.jsonl\"),\n",
        "  dev_set_format0, dev_labels\n",
        ")\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format0\", \"test.jsonl\"),\n",
        "  test_set_format0, test_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25188\\3560209190.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_vncorenlp_wseg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_set_format1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.8\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25188\\4003989162.py\u001b[0m in \u001b[0;36mformat\u001b[1;34m(self, format_code, run_vncorenlp_wseg)\u001b[0m\n\u001b[0;32m     85\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_vncorenlp_wseg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_vncorenlp_wseg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munformatted_offset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrun_vncorenlp_wseg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_vncorenlp_wseg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munformatted_offset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25188\\4003989162.py\u001b[0m in \u001b[0;36m_format\u001b[1;34m(self, dataset_offset)\u001b[0m\n\u001b[0;32m    267\u001b[0m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Preprocessor is using some non-predefined format code.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5615\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5616\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5617\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "preprocessor.reset_format()\n",
        "preprocessor.format(format_code=1, run_vncorenlp_wseg=True)\n",
        "preprocessor.shuffle(42)\n",
        "\n",
        "train_set_format1 = preprocessor.sentences[:int(.8 * n)]\n",
        "dev_set_format1 = preprocessor.sentences[int(.8 * n):int(.9 * n)]\n",
        "test_set_format1 = preprocessor.sentences[int(.9 * n):]\n",
        "\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format1\", \"train.jsonl\"),\n",
        "  train_set_format1, train_labels\n",
        ")\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format1\", \"dev.jsonl\"),\n",
        "  dev_set_format1, dev_labels\n",
        ")\n",
        "save_data(\n",
        "  os.path.join(\"vlsp_preprocessed\", \"format1\", \"test.jsonl\"),\n",
        "  test_set_format1, test_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The length of each dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 3228\n",
            "Number of development samples: 404\n",
            "Number of testing samples: 404\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of training samples: {len(train_labels)}\")\n",
        "print(f\"Number of development samples: {len(dev_labels)}\")\n",
        "print(f\"Number of testing samples: {len(test_labels)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some of the samples in format 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['<location1/> <sep> <person1/> <sep> Tổng_Lãnh_sự <location1/> tại Việt_Nam - bà <person1/> - cho biết , bà rất thích ăn trái thanh_long Việt_Nam và bà vui_mừng khi gia_đình , bạn_bè bà ở Úc rồi_đây cũng được thưởng_thức loại trái_cây tuyệt_vời này của Việt_Nam .',\n",
              "       '<location1/> <sep> <location2/> <sep> Nạn_nhân trong vụ án này là bà Phạm_Thị_Ngọc_Diệp ( SN 1978 , trú <location2/> , <location1/> ) , là giáo_viên Trường THCS Chu_Văn_An , thị_trấn Chư_Sê , đồng_thời là vợ của ông Giáp_Bá_Dự - Chánh toà hình_sự TAND tỉnh Gia_Lai .',\n",
              "       '<location1/> <sep> <location2/> <sep> Khi đó , trên đường có xe đầu kéo 77 C-143 . 82 kéo sơ_mi rơ moóc 77 R - 022.74 do anh Hồ_Thanh_Lợi ( 38 tuổi , ngụ tổ 8 , <location2/> , <location1/> , tỉnh Bình_Định ) điều_khiển đang đi theo hướng Bắc-Nam.',\n",
              "       '<location1/> <sep> <person1/> <sep> <person1/> là nữ ca_sĩ trẻ triển_vọng của làng âm_nhạc <location1/> .',\n",
              "       '<organization1/> <sep> <person1/> <sep> Còn chị <person1/> là công_nhân của <organization1/> .',\n",
              "       \"<location1/> <sep> <organization1/> <sep> ( VOV ) Tóc_Tiên ' bóc_mẽ ' thương_hiệu Moschino đụng ý_tưởng với <organization1/> <location1/> mà cô từng mặc từ 1 năm trước .\",\n",
              "       '<organization1/> <sep> <person1/> <sep> Bên cạnh đó , <organization1/> cũng đang tăng_cường các điều_kiện vật_chất , đội_ngũ để việc triển_khai chương_trình mới ; trong đó điều quan_trọng nhất là phát_triển năng_lực đội_ngũ qua bồi_dưỡng , tập_huấn ” – Thứ_trưởng <person1/> chia_sẻ .',\n",
              "       '<location1/> <sep> <location2/> <sep> Trước đó , khoảng 23h40 ngày 19/9 , người_dân sống trên đường Bàn_Cờ <location2/> , <location1/> ) phát_hiện khói_lửa bốc lên tại cửa_hàng kinh_doanh túi_xách thời_trang ở số 59 nên hô_hoán nhau cùng dập lửa , ứng_cứu .',\n",
              "       '<location1/> <sep> <location2/> <sep> Qua đấu_tranh , Công_an huyện Anh_Sơn tiến_hành bắt khẩn_cấp chủ_quản_lý nhà_nghỉ là Nguyễn_Thị_Kế ( SN 1968 ) trú tại thôn Khe_Choăng , <location2/> , <location1/> về hành_vi chứa mại_dâm .',\n",
              "       '<location1/> <sep> <location2/> <sep> Về chất_lượng bánh_tráng <location2/> và các thôn khác trong xã , huyện , <location1/> đều như nhau vì công_thức làm giống nhau và đều được làm từ bột gạo .'],\n",
              "      dtype='<U3135')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev_set_format0[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some of the samples in format 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Tổng Lãnh sự <location> Úc </location> tại Việt Nam - bà <person> Karen Lanyon </person> - cho biết, bà rất thích ăn trái thanh long Việt Nam và bà vui mừng khi gia đình, bạn bè bà ở Úc rồi đây cũng được thưởng thức loại trái cây tuyệt vời này của Việt Nam .',\n",
              "       'Nạn nhân trong vụ án này là bà Phạm Thị Ngọc Diệp (SN 1978, trú <location> phường Tây Sơn </location> , <location> TP Pleiku </location> ), là giáo viên Trường THCS Chu Văn An , thị trấn Chư Sê , đồng thời là vợ của ông Giáp Bá Dự -Chánh tòa hình sự TAND tỉnh Gia Lai .',\n",
              "       'Khi đó, trên đường có xe đầu kéo 77C-143.82 kéo sơ mi rơ moóc 77R- 022.74 do anh Hồ Thanh Lợi (38 tuổi, ngụ tổ 8 , <location> phường Bùi Thị Xuân </location> , <location> TP.Quy Nhơn </location> , tỉnh Bình Định ) điều khiển đang đi theo hướng Bắc-Nam.',\n",
              "       '<person> Võ Kiều Vân </person> là nữ ca sĩ trẻ triển vọng của làng âm nhạc <location> Việt Nam </location> .',\n",
              "       'Còn chị <person> Tuyết </person> là công nhân của <organization> Công ty Samsung </organization> .',\n",
              "       \"(VOV ) Tóc Tiên 'bóc mẽ' thương hiệu Moschino đụng ý tưởng với <organization> ELLE </organization> <location> Việt Nam </location> mà cô từng mặc từ 1 năm trước.\",\n",
              "       'Bên cạnh đó, <organization> Bộ GD&ĐT </organization> cũng đang tăng cường các điều kiện vật chất, đội ngũ để việc triển khai chương trình mới; trong đó điều quan trọng nhất là phát triển năng lực đội ngũ qua bồi dưỡng, tập huấn” – Thứ trưởng <person> Nguyễn Thị Nghĩa </person> chia sẻ.',\n",
              "       'Trước đó, khoảng 23h40 ngày 19/9, người dân sống trên đường Bàn Cờ <location> (quận 3 </location> , <location> TP.HCM </location> ) phát hiện khói lửa bốc lên tại cửa hàng kinh doanh túi xách thời trang ở số 59 nên hô hoán nhau cùng dập lửa, ứng cứu.',\n",
              "       'Qua đấu tranh, Công an huyện Anh Sơn tiến hành bắt khẩn cấp chủ quản lý nhà nghỉ là Nguyễn Thị Kế (SN 1968) trú tại thôn Khe Choăng , <location> xã Châu Khê </location> , <location> huyện Con Cuông </location> về hành vi chứa mại dâm.',\n",
              "       'Về chất lượng bánh tráng <location> Kim Tây </location> và các thôn khác trong xã, huyện, <location> tỉnh Bình Định </location> đều như nhau vì công thức làm giống nhau và đều được làm từ bột gạo.'],\n",
              "      dtype='<U2887')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev_set_format1[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Statistics by labels in train set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'AFFILIATION': 0, 'LOCATED': 1, 'PART – WHOLE': 2, 'PERSONAL - SOCIAL': 3}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th># samples</th>\n",
              "      <th>Ratio (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1335</td>\n",
              "      <td>41.356877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>987</td>\n",
              "      <td>30.576208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>754</td>\n",
              "      <td>23.358116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>152</td>\n",
              "      <td>4.708798</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   # samples  Ratio (%)\n",
              "2       1335  41.356877\n",
              "0        987  30.576208\n",
              "1        754  23.358116\n",
              "3        152   4.708798"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(preprocessor.label2id)\n",
        "df = pd.DataFrame(pd.Series(train_labels).value_counts(), columns=[\"# samples\"])\n",
        "df[\"Ratio (%)\"] = df[\"# samples\"] / df[\"# samples\"].sum() * 100\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Statistics by labels in dev set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'AFFILIATION': 0, 'LOCATED': 1, 'PART – WHOLE': 2, 'PERSONAL - SOCIAL': 3}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th># samples</th>\n",
              "      <th>Ratio (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>154</td>\n",
              "      <td>38.118812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>135</td>\n",
              "      <td>33.415842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>82</td>\n",
              "      <td>20.297030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33</td>\n",
              "      <td>8.168317</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   # samples  Ratio (%)\n",
              "2        154  38.118812\n",
              "0        135  33.415842\n",
              "1         82  20.297030\n",
              "3         33   8.168317"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(preprocessor.label2id)\n",
        "df = pd.DataFrame(pd.Series(dev_labels).value_counts(), columns=[\"# samples\"])\n",
        "df[\"Ratio (%)\"] = df[\"# samples\"] / df[\"# samples\"].sum() * 100\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Statistics by labels in test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'AFFILIATION': 0, 'LOCATED': 1, 'PART – WHOLE': 2, 'PERSONAL - SOCIAL': 3}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th># samples</th>\n",
              "      <th>Ratio (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>173</td>\n",
              "      <td>42.821782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>128</td>\n",
              "      <td>31.683168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>92</td>\n",
              "      <td>22.772277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>2.722772</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   # samples  Ratio (%)\n",
              "2        173  42.821782\n",
              "0        128  31.683168\n",
              "1         92  22.772277\n",
              "3         11   2.722772"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(preprocessor.label2id)\n",
        "df = pd.DataFrame(pd.Series(test_labels).value_counts(), columns=[\"# samples\"])\n",
        "df[\"Ratio (%)\"] = df[\"# samples\"] / df[\"# samples\"].sum() * 100\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All self-relation errors the preprocessor detected:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('PART – WHOLE', '1-321', 'VLSP2020\\\\VLSP2020_RE_train\\\\23351965.conll'),\n",
              " ('PART – WHOLE', '1-161', 'VLSP2020\\\\VLSP2020_RE_train\\\\23352701.conll'),\n",
              " ('PART – WHOLE', '1-885', 'VLSP2020\\\\VLSP2020_RE_train\\\\23352753.conll'),\n",
              " ('PART – WHOLE', '1-41', 'VLSP2020\\\\VLSP2020_RE_train\\\\23353786.conll'),\n",
              " ('PART – WHOLE', '1-390', 'VLSP2020\\\\VLSP2020_RE_train\\\\23353891.conll'),\n",
              " ('PART – WHOLE', '1-350', 'VLSP2020\\\\VLSP2020_RE_train\\\\23354619.conll'),\n",
              " ('PART – WHOLE', '1-423', 'VLSP2020\\\\VLSP2020_RE_train\\\\23354619.conll'),\n",
              " ('PART – WHOLE', '1-1375', 'VLSP2020\\\\VLSP2020_RE_train\\\\23354880.conll'),\n",
              " ('PART – WHOLE', '1-151', 'VLSP2020\\\\VLSP2020_RE_dev\\\\23352337.conll'),\n",
              " ('PART – WHOLE', '1-393', 'VLSP2020\\\\VLSP2020_RE_dev\\\\23352396.conll'),\n",
              " ('PART – WHOLE', '1-197', 'VLSP2020\\\\VLSP2020_RE_dev\\\\23352445.conll'),\n",
              " ('PART – WHOLE', '1-741', 'VLSP2020\\\\VLSP2020_RE_dev\\\\23352491.conll'),\n",
              " ('AFFILIATION', '1-208', 'VLSP2020\\\\VLSP2020_RE_dev\\\\23352585.conll')]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocessor.self_relations"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
