{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzmyfWAy0pJw"
      },
      "source": [
        "# VLSP Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4qltZcTHBKS",
        "outputId": "081cc2c9-ad87-4d25-a4bc-d9ec3da9fba7"
      },
      "outputs": [],
      "source": [
        "import py_vncorenlp\n",
        "import os\n",
        "\n",
        "vncorenlp_dir = \"D:/Projects/albert-imdb/vncorenlp\"\n",
        "project_dir = \"D:/Projects/albert-imdb\"\n",
        "\n",
        "\n",
        "# uncomment this line if VnCoreNLP has not been downloaded\n",
        "# py_vncorenlp.download_model(save_dir=vncorenlp_path)\n",
        "\n",
        "# load VnCoreNLP\n",
        "vncorenlp_model = py_vncorenlp.VnCoreNLP(save_dir=vncorenlp_dir, annotators=[\"wseg\"])\n",
        "\n",
        "# change directory back to project\n",
        "os.chdir(project_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessor Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 7 annotation errors that needs fixing:\n",
        "1. Entity with punctuation: Normally, the annotation splits each word into each row by space. However if a word is followed immediately by a punctuation and that word is a part of an entity, the annotator will duplicate that word into the next row.\n",
        "```\n",
        "# 23357000.conll\n",
        "1-173\t807-810\tvăn\t*[8]\tORGANIZATION[8]\tAFFILIATION\t1-166[7_8]\n",
        "1-174\t811-816\tphòng\t*[8]\tORGANIZATION[8]\t_\t_\n",
        "1-175\t817-821\tkiến\t*[8]\tORGANIZATION[8]\t_\t_\n",
        "1-176\t822-826\ttrúc\t*[8]\tORGANIZATION[8]\t_\t_\n",
        "1-177\t827-833\t1+1>2;\t_\t_\t_\t_\n",
        "1-177.1\t827-832\t1+1>2\t*[8]\tORGANIZATION[8]\tAFFILIATION\t1-166[7_8]\n",
        "```\n",
        "2. Not separated by space: There are some cases that the annotator left two words with a space in between in a row. *(currently ignored the whole file)*\n",
        "```\n",
        "# 23352816.conll\n",
        "1-153\t756-763\t. VMISS\t*\tORGANIZATION\t_\t_\t_\n",
        "```\n",
        "3. Inter-sentence relations: The task is intra-sentence but there are still some relations between entities belong to different sentences.\n",
        "4. Relation annotation not only in the first row of the entity: If an entity is annotated with a relation, that relation should be inserted to the first row of that entity. But there are some cases, such as when getting the entity with punctuation error (see the example in that case), the entity annotation is interrupted and then continue with reinserting the relation.\n",
        "5. Relation not link to the first word of the entity: A relation should be linked to the first word of the other entity but when the other entity annotation is interrupted by the first error, the relation is linked to the row of the duplicated word with no punctuation.\n",
        "```\n",
        "# 23356574.conll\n",
        "1-742\t3340-3346\tchuyên\t*[18]\tORGANIZATION[18]\t_\t_\n",
        "1-743\t3347-3354\tnghiệp,\t_\t_\t_\t_\n",
        "1-743.1\t3347-3353\tnghiệp\t*[18]\tORGANIZATION[18]\tPART – WHOLE\t1-733[17_18]\n",
        "1-744\t3355-3357\tSở\t*[19]\tORGANIZATION[19]\tAFFILIATION|PART – WHOLE\t1-733[17_19]|1-743.1[18_19]\n",
        "1-745\t3358-3363\tGD-ĐT\t*[19]\tORGANIZATION[19]\t_\t_\n",
        "```\n",
        "6. Miscellaneous entities and relations:\n",
        "7. Same name for different entities: Normally, if there are two or more entities with the same type, there will be an index after the entity type to denote different entities. This error occurs when this practice is broken.\n",
        "```\n",
        "# 23351316.conll\n",
        "1-646\t2909-2913\tCông\t_\t_\t_\t_\t\n",
        "1-647\t2914-2916\tty\t_\t_\t_\t_\t\n",
        "1-648\t2917-2920\tcon\t_\t_\t_\t_\t\n",
        "1-649\t2921-2926\tWaymo\t*\tORGANIZATION\t_\t_\t\n",
        "1-650\t2927-2930\tcủa\t_\t_\t_\t_\t\n",
        "1-651\t2931-2939\tAlphabet\t*\tORGANIZATION\tPART – WHOLE\t1-649\t\n",
        "1-652\t2940-2943\tthì\t_\t_\t_\t_\t\n",
        "1-653\t2944-2948\tđang\t_\t_\t_\t_\t\n",
        "```\n",
        "There are some cases really hard to handle that we decide to ignore, e.g.:\n",
        "```\n",
        "23351965.conll\n",
        "1-320\t1419-1427\tBrussels\t*\tLOCATION\tPART – WHOLE\t1-321\t\n",
        "1-321\t1428-1431\t(Bỉ\t*\tLOCATION\t_\t_\t\n",
        "1-322\t1432-1434\t).\t_\t_\t_\t_\t\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "05LFe7LhCTci"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "\"\"\"\n",
        "format_code:\n",
        "\n",
        "0 for <entity1> <sep> <entity2> <sep> <sentence>\n",
        "where all instances of <entity1> and <entity2> are replaced by the corresponding tokens in <sentence>.\n",
        "\n",
        "1 for open tag and close tag for each entity are inserted into the sentence.\n",
        "\"\"\"\n",
        "class VlspPreprocessor:\n",
        "  special_token = {\n",
        "    # for format code 0\n",
        "    \"SEP\": \"<sep>\",\n",
        "    \"PERSON[1]\": \"<person1/>\",\n",
        "    \"PERSON[2]\": \"<person2/>\",\n",
        "    \"ORGANIZATION[1]\": \"<organization1/>\",\n",
        "    \"ORGANIZATION[2]\": \"<organization2/>\",\n",
        "    \"LOCATION[1]\": \"<location1/>\",\n",
        "    \"LOCATION[2]\": \"<location2/>\",\n",
        "    \n",
        "    # for format code 1\n",
        "    \"<PERSON>\": \"<person>\",\n",
        "    \"</PERSON>\": \"</person>\",\n",
        "    \"<ORGANIZATION>\": \"<organization>\",\n",
        "    \"</ORGANIZATION>\": \"</organization>\",\n",
        "    \"<LOCATION>\": \"<location>\",\n",
        "    \"</LOCATION>\": \"</location>\",\n",
        "  }\n",
        "  \n",
        "  eos_punctuation = [\".\", \"?\", \"!\"] # need updating end of sentence punctuations to be more legit\n",
        "  \n",
        "  def __init__(self, drop_no_relation_samples: bool=True, format_code: {0, 1}=0, run_vncorenlp_wseg: bool=False):\n",
        "    self.dataset = []\n",
        "    \n",
        "    self.label2id = {}\n",
        "    self.id2label = {}\n",
        "    self.drop_no_relation_samples = drop_no_relation_samples\n",
        "    \n",
        "    self.unformatted_offset = 0\n",
        "    self.format_code = format_code\n",
        "    self.run_vncorenlp_wseg = run_vncorenlp_wseg\n",
        "    \n",
        "    self.sentences = np.array([])\n",
        "    self.labels = np.array([], dtype=\"uint8\")\n",
        "    \n",
        "    self.self_relations = [] # for debugging error 7\n",
        "    \n",
        "  def __len__(self) -> int:\n",
        "    return len(self.sentences)\n",
        "  \n",
        "  def execute_all(self, src_dir: str, drop_no_relation_samples: bool=None, format_code: {0, 1}=None, run_vncorenlp_wseg: bool=None, shuffle: int=None):\n",
        "    print(f\"Executing {src_dir}...\")\n",
        "    self.load(src_dir, drop_no_relation_samples=drop_no_relation_samples)\n",
        "    print(\"Done loading.\")\n",
        "    self.format(format_code=format_code, run_vncorenlp_wseg=run_vncorenlp_wseg)\n",
        "    print(\"Done formatting.\")\n",
        "    if shuffle is not None:\n",
        "      self.shuffle(shuffle)\n",
        "      print(f\"Done shuffling with seed {shuffle}.\")\n",
        "    print(\"✅ Done all.\")\n",
        "    \n",
        "  def load(self, src_dir: str, drop_no_relation_samples: bool=None): # this function is designed to execute many times on many directories\n",
        "    if drop_no_relation_samples is not None:\n",
        "      self.drop_no_relation_samples = drop_no_relation_samples\n",
        "    \n",
        "    for root, _, files in os.walk(src_dir):\n",
        "      for file in files: # currently the structure is one tsv file per subfolder, but this loop is in case there are more\n",
        "        if os.path.join(\"VLSP2020_RE_train\", \"23352816.conll\") in root: # the tsv file in this subfolder is heavily corrupted, just ignore it for now\n",
        "          continue\n",
        "        if file.endswith(\".tsv\"):\n",
        "          self.root = root # for debugging\n",
        "          self.process_tsv(os.path.join(root, file))\n",
        "\n",
        "    if self.drop_no_relation_samples:\n",
        "      self._drop_no_relation_samples()\n",
        "    \n",
        "    self._build_id2label()\n",
        "  \n",
        "  def format(self, format_code: {0, 1}=None, run_vncorenlp_wseg: bool=None):\n",
        "    if format_code is not None:\n",
        "      self.format_code = format_code\n",
        "    if run_vncorenlp_wseg is not None:\n",
        "      self.run_vncorenlp_wseg = run_vncorenlp_wseg\n",
        "    \n",
        "    self._format(self.unformatted_offset)\n",
        "    if run_vncorenlp_wseg:\n",
        "      self._run_vncorenlp_wseg(self.unformatted_offset)\n",
        "    self.unformatted_offset = len(self.dataset)\n",
        "\n",
        "  def process_tsv(self, tsv_dir: str):\n",
        "    df = pd.read_csv(tsv_dir, sep=\"\\t\", comment=\"#\", quotechar=\"\\t\", header=None)\n",
        "    if len(df.columns) < 8: # relation columns are missing\n",
        "      return\n",
        "    df.columns = [\"ann_idx\", \"range\", \"word\", \"param\", \"entity\", \"relation\", \"rel_heads\", \"-\"]\n",
        "\n",
        "    self._handle_word_with_entity_subword(df)\n",
        "\n",
        "    dataset_offset = len(self.dataset)\n",
        "    self._extract_sentences(df)\n",
        "    # self._extract_entities(df, dataset_offset) # because of changes due to error 7, no need to keep track of entities anymore\n",
        "    self._extract_relations(df, dataset_offset)\n",
        "\n",
        "  def _handle_word_with_entity_subword(self, df):\n",
        "    error_indices = df[df[\"ann_idx\"].shift(-1).apply(lambda i: i is not None and \".1\" in str(i))][\"word\"].index\n",
        "    offset = 0\n",
        "    for idx in error_indices:\n",
        "      idx += offset\n",
        "      entity_word = df.iloc[idx + 1][\"word\"]\n",
        "      prefix, suffix = df.iloc[idx][\"word\"].split(entity_word, 1)\n",
        "\n",
        "      if idx > 0 and df.iloc[idx - 1][\"entity\"] == df.iloc[idx + 1][\"entity\"]:\n",
        "        df.loc[idx + 1, \"relation\"] = \"_\"\n",
        "        df.loc[idx + 1, \"rel_heads\"] = \"_\"\n",
        "\n",
        "      if suffix != \"\":\n",
        "        df.loc[idx + 1.5] = [\"_\", \"_\", suffix, \"_\", \"_\", \"_\", \"_\", np.nan]\n",
        "        offset += 1\n",
        "\n",
        "      if prefix != \"\":\n",
        "        df.loc[idx, \"word\"] = prefix\n",
        "      else:\n",
        "        df = df.drop(idx)\n",
        "        offset -= 1\n",
        "\n",
        "      df = df.sort_index().reset_index(drop=True)\n",
        "\n",
        "  def _extract_sentences(self, df):\n",
        "    sentence = []\n",
        "    for word in df[\"word\"].values:\n",
        "      word = str(word) #  in VLSP2020_RE_dev/23352623.conll, the word \"nan\" counts as a float ¯\\_(ツ)_/¯\n",
        "      sentence.append(word)\n",
        "      if word[-1] in VlspPreprocessor.eos_punctuation:\n",
        "        self.dataset.append({ \"word_list\": np.array(sentence) })\n",
        "        sentence = []\n",
        "    self.dataset.append({ \"word_list\": np.array(sentence) })\n",
        "\n",
        "  def _extract_entities(self, df, dataset_offset: int): # deprecated\n",
        "    offset = 0\n",
        "    sample_idx = dataset_offset\n",
        "    entity_df = df[df[\"entity\"] != \"_\"][\"entity\"]\n",
        "    for entity, idx in zip(entity_df.values, entity_df.index):\n",
        "      while idx >= offset + len(self.dataset[sample_idx][\"word_list\"]):\n",
        "        offset += len(self.dataset[sample_idx][\"word_list\"])\n",
        "        sample_idx += 1\n",
        "\n",
        "      if \"entities\" not in self.dataset[sample_idx]:\n",
        "        self.dataset[sample_idx][\"entities\"] = {} # set of entity variables\n",
        "\n",
        "      if entity not in self.dataset[sample_idx][\"entities\"]\\\n",
        "        and \"MISCELLANEOUS\" not in entity: # error 6, ignore miscellaneous entities\n",
        "        self.dataset[sample_idx][\"entities\"].update(entity)\n",
        "\n",
        "  def _extract_relations(self, df, dataset_offset: int):\n",
        "    offset = 0\n",
        "    sample_idx = dataset_offset\n",
        "    relation_df = df[df[\"relation\"] != \"_\"][[\"relation\", \"entity\", \"rel_heads\"]]    \n",
        "    for (relations, entity, rel_heads), idx in zip(relation_df.values, relation_df.index):\n",
        "      if \"MISCELLANEOUS\" in entity: # error 6, ignore miscellaneous entity\n",
        "        continue\n",
        "      \n",
        "      relations = relations.split(\"|\")\n",
        "      rel_heads = rel_heads.split(\"|\")\n",
        "        \n",
        "      while idx >= offset + len(self.dataset[sample_idx][\"word_list\"]):\n",
        "        offset += len(self.dataset[sample_idx][\"word_list\"])\n",
        "        sample_idx += 1\n",
        "      sample = self.dataset[sample_idx]\n",
        "      entity_range = self._find_range(df, idx, offset=offset)\n",
        "      entity = entity.split(\"[\")[0]\n",
        "      \n",
        "      for i in range(len(relations)):\n",
        "        if relations[i] == \"*\": # error 6, ignore miscellaneous relations in VLSP2020_RE_train\\23351515.conll and 23351856.conll\n",
        "          continue\n",
        "\n",
        "        other_entity_df = df[df[\"ann_idx\"] == rel_heads[i].split(\"[\")[0]][\"entity\"]\n",
        "        other_entity = other_entity_df.values[0].split(\"[\")[0]\n",
        "        other_entity_idx = other_entity_df.index[0]\n",
        "        \n",
        "        if \"MISCELLANEOUS\" in other_entity: # error 6, ignore miscellaneous entity\n",
        "          continue\n",
        "        if offset > other_entity_idx or other_entity_idx >= offset + len(sample[\"word_list\"]): # error 3, ignore inter-sentence relations\n",
        "          continue\n",
        "        \n",
        "        if \"relations\" not in sample:\n",
        "          sample[\"relations\"] = []\n",
        "        \n",
        "        other_entity_range = self._find_range(df, other_entity_idx, offset=offset)\n",
        "        \n",
        "        if entity_range[0] == other_entity_range[0] and  entity_range[1] == other_entity_range[1]: # error 7, ignore and save to debug log\n",
        "          self.self_relations.append((relations[i], rel_heads[i], self.root))\n",
        "          continue\n",
        "        \n",
        "        assert entity_range[1] <= other_entity_range[0] or other_entity_range[1] <= entity_range[0]\n",
        "        sample[\"relations\"].append((entity, entity_range, other_entity, other_entity_range, relations[i]))\n",
        "\n",
        "        if relations[i] not in self.label2id: # update label2id mapping\n",
        "          self.label2id[relations[i]] = len(self.label2id)\n",
        "          \n",
        "  def _find_range(self, df: pd.DataFrame, idx: int, offset: int=0) -> tuple[int, int]:\n",
        "    entity = df.iloc[int(idx)][\"entity\"]\n",
        "    x = int(idx)\n",
        "    while x > 0 and df.iloc[x - 1][\"entity\"] == entity:\n",
        "      x -= 1\n",
        "    y = int(idx) + 1\n",
        "    while y < df.shape[0] and df.iloc[y][\"entity\"] == entity:\n",
        "      y += 1\n",
        "    assert x - offset < y - offset\n",
        "    return (x - offset, y - offset)\n",
        "\n",
        "  def _drop_no_relation_samples(self, dataset_offset: int=0):\n",
        "    clean_dataset = []\n",
        "    for sample in self.dataset[dataset_offset:]:\n",
        "      if \"relations\" in sample:\n",
        "        clean_dataset.append(sample)\n",
        "    self.dataset = [*self.dataset[:dataset_offset], *clean_dataset]\n",
        "  \n",
        "  def _build_id2label(self):\n",
        "    self.id2label = {v: k for k, v in self.label2id.items()}\n",
        "  \n",
        "  def _format(self, dataset_offset: int):\n",
        "    for sample in self.dataset[dataset_offset:]:\n",
        "      if \"relations\" not in sample:\n",
        "        continue\n",
        "      \n",
        "      for ent1, ent1_range, ent2, ent2_range, rel_type in sample[\"relations\"]:\n",
        "        self.labels = np.append(self.labels, self.label2id[rel_type])\n",
        "        sentence = np.array([])\n",
        "        \n",
        "        if self.format_code == 0:\n",
        "          ent1 = f\"{ent1}[1]\"\n",
        "          ent2 = f\"{ent2}[{2 if ent1 == ent2 else 1}]\"\n",
        "          sentence = np.append(sentence, [\n",
        "            VlspPreprocessor.special_token[ent1],\n",
        "            VlspPreprocessor.special_token[\"SEP\"],\n",
        "            VlspPreprocessor.special_token[ent2],\n",
        "            VlspPreprocessor.special_token[\"SEP\"],\n",
        "          ])\n",
        "        \n",
        "        if ent1_range[0] >= ent2_range[1]:\n",
        "          ent1, ent1_range, ent2, ent2_range = ent2, ent2_range, ent1, ent1_range\n",
        "        assert ent1_range[1] <= ent2_range[0]\n",
        "        \n",
        "        if self.format_code == 0:\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][:ent1_range[0]])\n",
        "          sentence = np.append(sentence, [VlspPreprocessor.special_token[ent1]])\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][ent1_range[1]:ent2_range[0]])\n",
        "          sentence = np.append(sentence, [VlspPreprocessor.special_token[ent2]])\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][ent2_range[1]:])\n",
        "        elif self.format_code == 1:\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][:ent1_range[0]])\n",
        "          sentence = np.append(sentence, [VlspPreprocessor.special_token[f\"<{ent1}>\"]])\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][ent1_range[0]:ent1_range[1]])\n",
        "          sentence = np.append(sentence, [VlspPreprocessor.special_token[f\"</{ent1}>\"]])\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][ent1_range[1]:ent2_range[0]])\n",
        "          sentence = np.append(sentence, [VlspPreprocessor.special_token[f\"<{ent2}>\"]])\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][ent2_range[0]:ent2_range[1]])\n",
        "          sentence = np.append(sentence, [VlspPreprocessor.special_token[f\"</{ent2}>\"]])\n",
        "          sentence = np.append(sentence, sample[\"word_list\"][ent2_range[1]:])\n",
        "        else:\n",
        "          raise ValueError(\"Preprocessor is using some non-predefined format code.\")\n",
        "        \n",
        "        self.sentences = np.append(self.sentences, \" \".join(sentence))\n",
        "        \n",
        "    assert len(self.sentences) == len(self.labels)\n",
        "\n",
        "  def _run_vncorenlp_wseg(self, dataset_offset: int=0):\n",
        "    def sentence_transform(s: str):\n",
        "      s = vncorenlp_model.word_segment(s)[0]\n",
        "      for _, token in VlspPreprocessor.special_token.items():\n",
        "        wseg_token = vncorenlp_model.word_segment(token)[0]\n",
        "        s = s.replace(wseg_token, token)\n",
        "      return s\n",
        "\n",
        "    array_transform = np.vectorize(sentence_transform)\n",
        "    self.sentences = np.concatenate([\n",
        "      self.sentences[:dataset_offset],\n",
        "      array_transform(self.sentences[dataset_offset:])\n",
        "    ])\n",
        "  \n",
        "  def shuffle(self, seed: int=42):\n",
        "    np.random.seed(seed)\n",
        "    mask = np.random.permutation(len(self.sentences))\n",
        "    assert len(self.sentences) == len(self.labels)\n",
        "    self.sentences = self.sentences[mask]\n",
        "    self.labels = self.labels[mask]\n",
        "  \n",
        "  def clear(self):\n",
        "    self.dataset.clear()\n",
        "    self.label2id.clear()\n",
        "    self.id2label.clear()\n",
        "    self.reset_format()\n",
        "    \n",
        "  def reset_format(self):\n",
        "    self.unformatted_offset = 0\n",
        "    self.sentences = np.array([])\n",
        "    self.labels = np.array([], dtype=\"uint8\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "fLHwChL1UAlI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing VLSP2020\\VLSP2020_RE_train...\n",
            "Done loading.\n",
            "Done formatting.\n",
            "Done shuffling with seed 42.\n",
            "✅ Done all.\n"
          ]
        }
      ],
      "source": [
        "dir = os.path.join(\"VLSP2020\", \"VLSP2020_RE_train\")\n",
        "preprocessor = VlspPreprocessor()\n",
        "preprocessor.execute_all(dir, format_code=0, run_vncorenlp_wseg=True, shuffle=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "data = {\n",
        "  \"dataset\": list(preprocessor.sentences[:2000]),\n",
        "  \"labels\": [int(x) for x in preprocessor.labels[:2000]],\n",
        "  \"num_samples\": len(preprocessor.sentences[:2000]),\n",
        "  \"shuffle_seed\": 42,\n",
        "}\n",
        "\n",
        "with open(os.path.join(\"vlsp_preprocessed\", \"format0\", \"train.json\"), \"w\") as file:\n",
        "  json.dump(json.dumps(data), file)\n",
        "  \n",
        "data = {\n",
        "  \"dataset\": list(preprocessor.sentences[2000:]),\n",
        "  \"labels\": [int(x) for x in preprocessor.labels[2000:]],\n",
        "  \"num_samples\": len(preprocessor.sentences[2000:]),\n",
        "  \"shuffle_seed\": 42,\n",
        "}\n",
        "\n",
        "with open(os.path.join(\"vlsp_preprocessed\", \"format0\", \"dev.json\"), \"w\") as file:\n",
        "  json.dump(json.dumps(data), file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessor.reset_format()\n",
        "preprocessor.format(format_code=1, run_vncorenlp_wseg=True)\n",
        "preprocessor.shuffle(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = {\n",
        "  \"dataset\": list(preprocessor.sentences[:2000]),\n",
        "  \"labels\": [int(x) for x in preprocessor.labels[:2000]],\n",
        "  \"num_samples\": len(preprocessor.sentences[:2000]),\n",
        "  \"shuffle_seed\": 42,\n",
        "}\n",
        "\n",
        "with open(os.path.join(\"vlsp_preprocessed\", \"format1\", \"train.json\"), \"w\") as file:\n",
        "  json.dump(json.dumps(data), file)\n",
        "  \n",
        "data = {\n",
        "  \"dataset\": list(preprocessor.sentences[2000:]),\n",
        "  \"labels\": [int(x) for x in preprocessor.labels[2000:]],\n",
        "  \"num_samples\": len(preprocessor.sentences[2000:]),\n",
        "  \"shuffle_seed\": 42,\n",
        "}\n",
        "\n",
        "with open(os.path.join(\"vlsp_preprocessed\", \"format1\", \"dev.json\"), \"w\") as file:\n",
        "  json.dump(json.dumps(data), file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing VLSP2020\\VLSP2020_RE_dev...\n",
            "Done loading.\n",
            "Done formatting.\n",
            "Done shuffling with seed 42.\n",
            "✅ Done all.\n"
          ]
        }
      ],
      "source": [
        "preprocessor.clear()\n",
        "dir = os.path.join(\"VLSP2020\", \"VLSP2020_RE_dev\")\n",
        "preprocessor.execute_all(dir, format_code=0, run_vncorenlp_wseg=True, shuffle=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = {\n",
        "  \"dataset\": list(preprocessor.sentences),\n",
        "  \"labels\": [int(x) for x in preprocessor.labels],\n",
        "  \"num_samples\": len(preprocessor.sentences),\n",
        "  \"shuffle_seed\": 42,\n",
        "}\n",
        "\n",
        "with open(os.path.join(\"vlsp_preprocessed\", \"format0\", \"test.json\"), \"w\") as file:\n",
        "  json.dump(json.dumps(data), file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessor.reset_format()\n",
        "preprocessor.format(format_code=1, run_vncorenlp_wseg=True)\n",
        "preprocessor.shuffle(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = {\n",
        "  \"dataset\": list(preprocessor.sentences),\n",
        "  \"labels\": [int(x) for x in preprocessor.labels],\n",
        "  \"num_samples\": len(preprocessor.sentences),\n",
        "  \"shuffle_seed\": 42,\n",
        "}\n",
        "\n",
        "with open(os.path.join(\"vlsp_preprocessed\", \"format1\", \"test.json\"), \"w\") as file:\n",
        "  json.dump(json.dumps(data), file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
