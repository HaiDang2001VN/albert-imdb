{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7HpRMs3_FsJ"
      },
      "outputs": [],
      "source": [
        "!unzip -q VLSP2020_RE_train.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzmyfWAy0pJw"
      },
      "source": [
        "## Preprocessor\n",
        "\n",
        "There are 5 annotation errors that needs fixing:\n",
        "1. Entity with punctuation: Normally, the annotation splits each word into each row by space. However if a word is followed immediately by a punctuation and that word is a part of an entity, the annotator will duplicate that word into the next row.\n",
        "```\n",
        "# 23357000.conll\n",
        "1-173\t807-810\tvăn\t*[8]\tORGANIZATION[8]\tAFFILIATION\t1-166[7_8]\n",
        "1-174\t811-816\tphòng\t*[8]\tORGANIZATION[8]\t_\t_\n",
        "1-175\t817-821\tkiến\t*[8]\tORGANIZATION[8]\t_\t_\n",
        "1-176\t822-826\ttrúc\t*[8]\tORGANIZATION[8]\t_\t_\n",
        "1-177\t827-833\t1+1>2;\t_\t_\t_\t_\n",
        "1-177.1\t827-832\t1+1>2\t*[8]\tORGANIZATION[8]\tAFFILIATION\t1-166[7_8]\n",
        "```\n",
        "2. Not separated by space: There are some cases that the annotator left two words with a space in between in a row. *(currently ignored the whole file)*\n",
        "```\n",
        "# 23352816.conll\n",
        "1-153\t756-763\t. VMISS\t*\tORGANIZATION\t_\t_\t_\n",
        "```\n",
        "3. Inter-sentence relations: The task is intra-sentence but there are still some relations between entities belong to different sentences.\n",
        "4. Relation annotation not only in the first row of the entity: If an entity is annotated with a relation, that relation should be inserted to the first row of that entity. But there are some cases, such as when getting the entity with punctuation error (see the example in that case), the entity annotation is interrupted and then continue with reinserting the relation.\n",
        "5. Relation not link to the first word of the entity: A relation should be linked to the first word of the other entity but when the other entity annotation is interrupted by the first error, the relation is linked to the row of the duplicated word with no punctuation.\n",
        "```\n",
        "1-738\t3318-3324\tTrưởng\t*[18]\tORGANIZATION[18]\tPART – WHOLE\t1-733[17_18]\n",
        "1-739\t3325-3330\tphòng\t*[18]\tORGANIZATION[18]\t_\t_\n",
        "1-740\t3331-3335\tGiáo\t*[18]\tORGANIZATION[18]\t_\t_\n",
        "1-741\t3336-3339\tdục\t*[18]\tORGANIZATION[18]\t_\t_\n",
        "1-742\t3340-3346\tchuyên\t*[18]\tORGANIZATION[18]\t_\t_\n",
        "1-743\t3347-3354\tnghiệp,\t_\t_\t_\t_\n",
        "1-743.1\t3347-3353\tnghiệp\t*[18]\tORGANIZATION[18]\tPART – WHOLE\t1-733[17_18]\n",
        "1-744\t3355-3357\tSở\t*[19]\tORGANIZATION[19]\tAFFILIATION|PART – WHOLE\t1-733[17_19]|1-743.1[18_19]\n",
        "1-745\t3358-3363\tGD-ĐT\t*[19]\tORGANIZATION[19]\t_\t_\n",
        "1-746\t3364-3368\ttỉnh\t*[19]\tORGANIZATION[19]\t_\t_\n",
        "1-747\t3369-3373\tNghệ\t*[19]\tORGANIZATION[19]\t_\t_\n",
        "1-748\t3374-3376\tAn\t*[19]\tORGANIZATION[19]\t_\t_\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "05LFe7LhCTci"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "final_format:\n",
        "\n",
        "0 for <entity1> <sep> <entity2> <sep> <sentence>\n",
        "where all instances of <entity1> and <entity2> are replaced by the corresponding tokens in <sentence>.\n",
        "\n",
        "1 for open tag and close tag for each entity are inserted into the sentence.\n",
        "\"\"\"\n",
        "\n",
        "class VlspPreprocessor:\n",
        "  def __init__(self, src_dir, drop_samples_with_no_relations=True, final_format=0):\n",
        "    self.dataset = []\n",
        "    self.label2id = {}\n",
        "\n",
        "    for root, _, files in os.walk(src_dir):\n",
        "      for file in files: # currently the structure is one tsv file per subfolder, but this loop is in case there are more\n",
        "        if os.path.join(\"VLSP2020_RE_train\", \"23352816.conll\") in root: # the tsv file in this subfolder is heavily corrupted, just drop it for now\n",
        "          return\n",
        "        if file.endswith(\".tsv\"):\n",
        "          self.root = root\n",
        "          self.process_tsv(os.path.join(root, file))\n",
        "\n",
        "    if drop_samples_with_no_relations:\n",
        "      self._drop_samples_with_no_relations()\n",
        "    self._handle_inter_sentence_relations()\n",
        "    self._build_id2label()\n",
        "\n",
        "  def process_tsv(self, tsv_dir):\n",
        "    df = pd.read_csv(tsv_dir, sep=\"\\t\", comment=\"#\", quotechar=\"\\t\", header=None)\n",
        "    if len(df.columns) < 8: # relation columns are missing\n",
        "      return\n",
        "    df.columns = [\"ann_idx\", \"range\", \"word\", \"param\", \"entity\", \"relation\", \"rel_heads\", \"-\"]\n",
        "\n",
        "    self._handle_word_with_entity_subword(df)\n",
        "\n",
        "    dataset_offset = len(self.dataset)\n",
        "    self._extract_sentences(df)\n",
        "    self._extract_entities(df, dataset_offset)\n",
        "    self._extract_relations(df, dataset_offset)\n",
        "\n",
        "  def _handle_word_with_entity_subword(self, df):\n",
        "    error_indices = df[df[\"ann_idx\"].shift(-1).apply(lambda i: i is not None and \".1\" in i)][\"word\"].index\n",
        "    offset = 0\n",
        "    for idx in error_indices:\n",
        "      idx += offset\n",
        "      entity_word = df.iloc[idx + 1][\"word\"]\n",
        "      prefix, suffix = df.iloc[idx][\"word\"].split(entity_word, 1)\n",
        "\n",
        "      if idx > 0 and df.iloc[idx - 1][\"entity\"] == df.iloc[idx + 1][\"entity\"]:\n",
        "        df.loc[idx + 1, \"relation\"] = \"_\"\n",
        "        df.loc[idx + 1, \"rel_heads\"] = \"_\"\n",
        "\n",
        "      if suffix != \"\":\n",
        "        df.loc[idx + 1.5] = [\"_\", \"_\", suffix, \"_\", \"_\", \"_\", \"_\", np.nan]\n",
        "        offset += 1\n",
        "\n",
        "      if prefix != \"\":\n",
        "        df.loc[idx, \"word\"] = prefix\n",
        "      else:\n",
        "        df = df.drop(idx)\n",
        "        offset -= 1\n",
        "\n",
        "      df = df.sort_index().reset_index(drop=True)\n",
        "\n",
        "  def _extract_sentences(self, df):\n",
        "    sentence = []\n",
        "    for word in df[\"word\"].values:\n",
        "      word = str(word) #  in VLSP2020_RE_dev/23352623.conll, the word \"nan\" counts as a float ¯\\_(ツ)_/¯\n",
        "      sentence.append(word)\n",
        "      if word.endswith(\".\"):\n",
        "        self.dataset.append({ \"sentence\": sentence })\n",
        "        sentence = []\n",
        "    self.dataset.append({ \"sentence\": sentence })\n",
        "\n",
        "  def _extract_entities(self, df, dataset_offset):\n",
        "    offset = 0\n",
        "    sample_idx = dataset_offset\n",
        "    entity_df = df[df[\"entity\"] != \"_\"][\"entity\"]\n",
        "    for entity, idx in zip(entity_df.values, entity_df.index):\n",
        "      while idx >= offset + len(self.dataset[sample_idx][\"sentence\"]):\n",
        "        offset += len(self.dataset[sample_idx][\"sentence\"])\n",
        "        sample_idx += 1\n",
        "\n",
        "      if \"entities\" not in self.dataset[sample_idx]:\n",
        "        self.dataset[sample_idx][\"entities\"] = {}\n",
        "\n",
        "      if entity in self.dataset[sample_idx][\"entities\"]:\n",
        "        self.dataset[sample_idx][\"entities\"][entity].append(idx - offset)\n",
        "      else:\n",
        "        self.dataset[sample_idx][\"entities\"][entity] = [idx - offset]\n",
        "\n",
        "  def _extract_relations(self, df, dataset_offset):\n",
        "    offset = 0\n",
        "    sample_idx = dataset_offset\n",
        "    relation_df = df[df[\"relation\"] != \"_\"][[\"relation\", \"entity\", \"rel_heads\"]]\n",
        "    for (relations, entity, rel_heads), idx in zip(relation_df.values, relation_df.index):\n",
        "      relations = relations.split(\"|\")\n",
        "      rel_heads = rel_heads.split(\"|\")\n",
        "      for i in range(len(relations)):\n",
        "        while idx >= offset + len(self.dataset[sample_idx][\"sentence\"]):\n",
        "          offset += len(self.dataset[sample_idx][\"sentence\"])\n",
        "          sample_idx += 1\n",
        "\n",
        "        if \"relations\" not in self.dataset[sample_idx]:\n",
        "          self.dataset[sample_idx][\"relations\"] = []\n",
        "\n",
        "        other_entity = df[df[\"ann_idx\"] == rel_heads[i].split(\"[\")[0]][\"entity\"].values[0]\n",
        "        self.dataset[sample_idx][\"relations\"].append((entity, other_entity, relations[i]))\n",
        "\n",
        "        if relations[i] not in self.label2id:\n",
        "          self.label2id[relations[i]] = len(self.label2id)\n",
        "\n",
        "        if relations[i] == \"*\":\n",
        "          print(sample_idx, dataset_offset, len(relations), rel_heads, self.root)\n",
        "          print((entity, other_entity, relations[i]))\n",
        "\n",
        "  def _handle_inter_sentence_relations(self, dataset_offset=0):\n",
        "    for sample in self.dataset[dataset_offset:]:\n",
        "      if \"relations\" not in sample:\n",
        "        continue\n",
        "      intra_relations = []\n",
        "      for x, y, r in sample[\"relations\"]:\n",
        "        if x in sample[\"entities\"] and y in sample[\"entities\"]:\n",
        "          intra_relations.append((x, y, r))\n",
        "      sample[\"relations\"] = intra_relations\n",
        "\n",
        "  def _drop_samples_with_no_relations(self, dataset_offset=0):\n",
        "    clean_dataset = []\n",
        "    for sample in self.dataset[dataset_offset:]:\n",
        "      if \"relations\" in sample:\n",
        "        clean_dataset.append(sample)\n",
        "    self.dataset = [*self.dataset[:dataset_offset], *clean_dataset]\n",
        "\n",
        "  def _build_id2label(self):\n",
        "    self.id2label = {v: k for k, v in self.label2id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fLHwChL1UAlI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "544 535 4 ['1-315[18_19]', '1-301[15_19]', '1-307[16_19]', '1-310[17_19]'] VLSP2020_RE_train\\VLSP2020\\VLSP2020_RE_train\\23351515.conll\n",
            "('LOCATION[19]', 'LOCATION[17]', '*')\n",
            "1668 1662 1 ['1-198[0_13]'] VLSP2020_RE_train\\VLSP2020\\VLSP2020_RE_train\\23351856.conll\n",
            "('LOCATION[13]', 'PERSON', '*')\n",
            "1668 1662 1 ['1-198[0_14]'] VLSP2020_RE_train\\VLSP2020\\VLSP2020_RE_train\\23351856.conll\n",
            "('LOCATION[14]', 'PERSON', '*')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'AFFILIATION': 0,\n",
              " 'LOCATED': 1,\n",
              " 'PART – WHOLE': 2,\n",
              " 'PERSONAL - SOCIAL': 3,\n",
              " '*': 4}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dir = os.path.join(\"VLSP2020_RE_train\", \"VLSP2020\", \"VLSP2020_RE_train\")\n",
        "preprocessor = VlspPreprocessor(dir)\n",
        "preprocessor.label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4qltZcTHBKS",
        "outputId": "081cc2c9-ad87-4d25-a4bc-d9ec3da9fba7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3376"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformer import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\", use_fast=False)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
